<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>K8S(kubernetes)实践认知 | 北青永恒</title><meta name="keywords" content="K8s"><meta name="author" content="Thaons"><meta name="copyright" content="Thaons"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="Kubernetes快速入门 学习Kubernetes最权威的知识来源就是Kubernetes官方文档，而且对于初学者来说，官方文档可能不是最佳选择。本章将带你循循序渐进的学习Kubernetes，后面章节会通过大量的实践案例来理解和掌握Kubernetes的知识。   * Kubernetes官方文档：https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;home&#x2F;  * Kuernetes G">
<meta property="og:type" content="article">
<meta property="og:title" content="K8S(kubernetes)实践认知">
<meta property="og:url" content="https://kococ.cn/posts/47211/index.html">
<meta property="og:site_name" content="北青永恒">
<meta property="og:description" content="Kubernetes快速入门 学习Kubernetes最权威的知识来源就是Kubernetes官方文档，而且对于初学者来说，官方文档可能不是最佳选择。本章将带你循循序渐进的学习Kubernetes，后面章节会通过大量的实践案例来理解和掌握Kubernetes的知识。   * Kubernetes官方文档：https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;home&#x2F;  * Kuernetes G">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/xoxoyun/img/raw/master/image/0122.jpg">
<meta property="article:published_time" content="2020-03-09T01:16:00.000Z">
<meta property="article:modified_time" content="2020-10-20T04:18:19.426Z">
<meta property="article:author" content="Thaons">
<meta property="article:tag" content="K8s">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/xoxoyun/img/raw/master/image/0122.jpg"><link rel="shortcut icon" href="https://www.kococ.cn/favicon.ico"><link rel="canonical" href="https://kococ.cn/posts/47211/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="preconnect" href="//zz.bdstatic.com"/><meta name="yandex-verification" content="{&quot;theme_color&quot;:{&quot;enable&quot;:true,&quot;main&quot;:&quot;#49B1F5&quot;,&quot;paginator&quot;:&quot;#00c4b6&quot;,&quot;button_hover&quot;:&quot;#FF7242&quot;,&quot;text_selection&quot;:&quot;#00c4b6&quot;,&quot;link_color&quot;:&quot;#99a9bf&quot;,&quot;meta_color&quot;:&quot;#858585&quot;,&quot;hr_color&quot;:&quot;#A4D8FA&quot;,&quot;code_foreground&quot;:&quot;#F47466&quot;,&quot;code_background&quot;:&quot;rgba(27, 31, 35, .05)&quot;,&quot;toc_color&quot;:&quot;#00c4b6&quot;,&quot;blockquote_padding_color&quot;:&quot;#49b1f5&quot;,&quot;blockquote_background_color&quot;:&quot;#49b1f5&quot;}}"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.0.0',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-10-20 12:18:19'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(function () {
  window.activateDarkMode = function () {
    document.documentElement.setAttribute('data-theme', 'dark')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
    }
  }
  window.activateLightMode = function () {
    document.documentElement.setAttribute('data-theme', 'light')
    if (document.querySelector('meta[name="theme-color"]') !== null) {
      document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
    }
  }

  const autoChangeMode = 'false'
  const t = saveToLocal.get('theme')
  if (autoChangeMode === '1') {
    const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
    const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
    const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
    const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

    if (t === undefined) {
      if (isLightMode) activateLightMode()
      else if (isDarkMode) activateDarkMode()
      else if (isNotSpecified || hasNoSupport) {
        const now = new Date()
        const hour = now.getHours()
        const isNight = hour <= 6 || hour >= 18
        isNight ? activateDarkMode() : activateLightMode()
      }
      window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
        if (saveToLocal.get('theme') === undefined) {
          e.matches ? activateDarkMode() : activateLightMode()
        }
      })
    } else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else if (autoChangeMode === '2') {
    const now = new Date()
    const hour = now.getHours()
    const isNight = hour <= 6 || hour >= 18
    if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
    else if (t === 'light') activateLightMode()
    else activateDarkMode()
  } else {
    if (t === 'dark') activateDarkMode()
    else if (t === 'light') activateLightMode()
  }
})()</script><link rel="stylesheet" href="/css/footer.css"><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 5.0.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://gitee.com/xoxoyun/img/raw/master/image/wxuser.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">76</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">79</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">8</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://music.kococ.cn"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.2020web.cn"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Kubernetes%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">Kubernetes快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kubernetes%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.</span> <span class="toc-text">Kubernetes架构介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kubernetes%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.1.</span> <span class="toc-text">Kubernetes系统架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kubernetes%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.2.</span> <span class="toc-text">Kubernetes逻辑架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kubernetes%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.1.3.</span> <span class="toc-text">Kubernetes网络介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8kubeadm%E9%83%A8%E7%BD%B2Kubernetes-v1-16-4"><span class="toc-number">1.2.</span> <span class="toc-text">使用kubeadm部署Kubernetes v1.16.4</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2Docker"><span class="toc-number">1.2.1.</span> <span class="toc-text">部署Docker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2kubadm%E5%92%8Ckubelet"><span class="toc-number">1.2.2.</span> <span class="toc-text">部署kubadm和kubelet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2Master"><span class="toc-number">1.2.3.</span> <span class="toc-text">初始化集群部署Master</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6"><span class="toc-number">1.2.4.</span> <span class="toc-text">部署网络插件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2Node%E8%8A%82%E7%82%B9"><span class="toc-number">1.2.5.</span> <span class="toc-text">部署Node节点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AC%E6%9C%89%E4%BA%91%E4%B8%AD%E7%9A%84Kubernetes"><span class="toc-number">1.3.</span> <span class="toc-text">公有云中的Kubernetes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%AD%E7%9A%84Kubernetes"><span class="toc-number">1.3.1.</span> <span class="toc-text">阿里云中的Kubernetes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A7%81%E6%9C%89%E4%BA%91%E4%B8%AD%E7%9A%84Kubernetes"><span class="toc-number">1.3.2.</span> <span class="toc-text">私有云中的Kubernetes</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E9%83%A8%E5%88%86-%E5%B0%86%E5%BA%94%E7%94%A8%E8%BF%81%E7%A7%BB%E8%87%B3Kubernetes"><span class="toc-number">2.</span> <span class="toc-text">第五部分 将应用迁移至Kubernetes</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%B0%86%E5%BA%94%E7%94%A8%E5%B0%81%E8%A3%85%E8%BF%9B%E5%AE%B9%E5%99%A8%E4%B8%AD"><span class="toc-number">3.</span> <span class="toc-text">6 第一步：将应用封装进容器中</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%B0%86%E5%BA%94%E7%94%A8%E5%B0%81%E8%A3%85%E8%BF%9B%E5%AE%B9%E5%99%A8%E4%B8%AD"><span class="toc-number">3.1.</span> <span class="toc-text">第一步：将应用封装进容器中</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2Harbor%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93"><span class="toc-number">3.1.1.</span> <span class="toc-text">部署Harbor镜像仓库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%B6%E4%BD%9C%E5%AE%9E%E9%AA%8C%E7%94%A8%E7%9A%84Docker%E9%95%9C%E5%83%8F"><span class="toc-number">3.1.2.</span> <span class="toc-text">制作实验用的Docker镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEDocker%E4%BB%93%E5%BA%93"><span class="toc-number">3.1.3.</span> <span class="toc-text">配置Docker仓库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%99%BB%E5%BD%95Harbor%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93"><span class="toc-number">3.1.4.</span> <span class="toc-text">登录Harbor镜像仓库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4%E9%95%9C%E5%83%8F%E5%88%B0Registry"><span class="toc-number">3.1.5.</span> <span class="toc-text">提交镜像到Registry</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E5%B0%86%E5%AE%B9%E5%99%A8%E5%B0%81%E8%A3%85%E5%88%B0Pod%E4%B8%AD"><span class="toc-number">4.</span> <span class="toc-text">7 第二步：将容器封装到Pod中</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pod%E7%AE%A1%E7%90%86"><span class="toc-number">4.1.</span> <span class="toc-text">Pod管理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-3-%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E4%BD%BF%E7%94%A8Controllers%E7%AE%A1%E7%90%86Pod"><span class="toc-number">5.</span> <span class="toc-text">7.3 第三步：使用Controllers管理Pod</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-1-Replication-Controller%E6%8E%A7%E5%88%B6%E5%99%A8"><span class="toc-number">6.</span> <span class="toc-text">8.1 Replication Controller控制器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-2-Replica-Sets%E6%8E%A7%E5%88%B6%E5%99%A8"><span class="toc-number">7.</span> <span class="toc-text">8.2 Replica Sets控制器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-3-Deployment%E6%8E%A7%E5%88%B6%E5%99%A8"><span class="toc-number">8.</span> <span class="toc-text">8.3 Deployment控制器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-4-DaemonSet%E6%8E%A7%E5%88%B6%E5%99%A8"><span class="toc-number">9.</span> <span class="toc-text">8.4 DaemonSet控制器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-4-%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E4%BD%BF%E7%94%A8Service%E7%AE%A1%E7%90%86Pod%E8%AE%BF%E9%97%AE"><span class="toc-number">10.</span> <span class="toc-text">7.4 第四步：使用Service管理Pod访问</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-1-Service%E4%BB%8B%E7%BB%8D%E5%92%8C%E7%AE%A1%E7%90%86"><span class="toc-number">11.</span> <span class="toc-text">9.1 Service介绍和管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAService"><span class="toc-number">11.0.1.</span> <span class="toc-text">创建Service</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-2-Service%E5%92%8CEndpoint"><span class="toc-number">12.</span> <span class="toc-text">9.2 Service和Endpoint</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAHeadless-Service"><span class="toc-number">12.0.1.</span> <span class="toc-text">创建一个Headless Service</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-5-%E7%AC%AC%E4%BA%94%E6%AD%A5%EF%BC%9A%E4%BD%BF%E7%94%A8Ingress%E6%8F%90%E4%BE%9B%E5%A4%96%E9%83%A8%E8%AE%BF%E9%97%AE"><span class="toc-number">13.</span> <span class="toc-text">7.5 第五步：使用Ingress提供外部访问</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#10-1-Ingress-Controller"><span class="toc-number">13.1.</span> <span class="toc-text">10.1 Ingress Controller</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ingress-Controller-Traefik"><span class="toc-number">13.2.</span> <span class="toc-text">Ingress Controller Traefik</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2Treafik"><span class="toc-number">13.2.1.</span> <span class="toc-text">部署Treafik</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-%E7%AC%AC%E5%85%AD%E6%AD%A5%EF%BC%9A%E4%BD%BF%E7%94%A8PV%E5%92%8CPVC%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="toc-number">14.</span> <span class="toc-text">11 第六步：使用PV和PVC管理数据存储</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-1-Kubernetes-Volume"><span class="toc-number">15.</span> <span class="toc-text">11.1 Kubernetes Volume</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-2-PersistentVolume%EF%BC%88PV%EF%BC%89"><span class="toc-number">16.</span> <span class="toc-text">11.2 PersistentVolume（PV）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-3-PersistentVolumeClaim%EF%BC%88PVC%EF%BC%89"><span class="toc-number">17.</span> <span class="toc-text">11.3 PersistentVolumeClaim（PVC）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#11-4-StorageClass"><span class="toc-number">18.</span> <span class="toc-text">11.4 StorageClass</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E7%AC%AC%E4%B8%83%E6%AD%A5%EF%BC%9A%E4%BD%BF%E7%94%A8Rancher%E7%AE%A1%E7%90%86Kubernetes%E9%9B%86%E7%BE%A4"><span class="toc-number">19.</span> <span class="toc-text">7 第七步：使用Rancher管理Kubernetes集群</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-Rancher%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-number">19.1.</span> <span class="toc-text">7.1 Rancher快速入门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E4%BD%BF%E7%94%A8Rancher%E8%BF%9B%E8%A1%8C%E6%97%A5%E5%B8%B8%E7%AE%A1%E7%90%86"><span class="toc-number">19.2.</span> <span class="toc-text">7.2 使用Rancher进行日常管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-Rancher%E7%94%9F%E4%BA%A7%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="toc-number">19.3.</span> <span class="toc-text">7.3 Rancher生产集群部署</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E9%83%A8%E5%88%86-%E7%AE%A1%E7%90%86Kubernetes%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">20.</span> <span class="toc-text">第六部分 管理Kubernetes中的应用</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-%E5%BA%94%E7%94%A8%E7%9A%84%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6%E5%92%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5"><span class="toc-number">21.</span> <span class="toc-text">13 应用的资源限制和健康检查</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-1-%E5%BA%94%E7%94%A8%E7%9A%84%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6"><span class="toc-number">22.</span> <span class="toc-text">13.1 应用的资源限制</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#13-2-%E5%BA%94%E7%94%A8%E7%9A%84%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5"><span class="toc-number">23.</span> <span class="toc-text">13.2 应用的健康检查</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Liveness%E6%8E%A2%E6%B5%8B"><span class="toc-number">23.0.1.</span> <span class="toc-text">Liveness探测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Readiness%E6%8E%A2%E6%B5%8B"><span class="toc-number">23.0.2.</span> <span class="toc-text">Readiness探测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">23.1.</span> <span class="toc-text">健康检查的方法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%A1%E7%90%86%E5%BA%94%E7%94%A8%E7%9A%84DNS%E8%AE%BF%E9%97%AE"><span class="toc-number">24.</span> <span class="toc-text">管理应用的DNS访问</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#14-1-Kubernetes%E4%B8%AD%E7%9A%84DNS"><span class="toc-number">25.</span> <span class="toc-text">14.1 Kubernetes中的DNS</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E7%9A%84DNS%E7%AE%A1%E7%90%86"><span class="toc-number">26.</span> <span class="toc-text">应用的DNS管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pod%E7%9A%84%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%AD%96%E7%95%A5"><span class="toc-number">26.0.1.</span> <span class="toc-text">Pod的域名解析策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E7%9A%84DNS%E7%AE%A1%E7%90%86-1"><span class="toc-number">26.1.</span> <span class="toc-text">应用的DNS管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-1-%E4%BD%BF%E7%94%A8ConfigMap%E7%AE%A1%E7%90%86%E5%BA%94%E7%94%A8%E9%85%8D%E7%BD%AE"><span class="toc-number">26.2.</span> <span class="toc-text">15.1 使用ConfigMap管理应用配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87kubectl%E5%91%BD%E4%BB%A4%E5%88%9B%E5%BB%BAConfigMap"><span class="toc-number">26.2.1.</span> <span class="toc-text">通过kubectl命令创建ConfigMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87YAML%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BAConfigMap"><span class="toc-number">26.2.2.</span> <span class="toc-text">通过YAML文件创建ConfigMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BAConfigMap"><span class="toc-number">26.2.3.</span> <span class="toc-text">通过文件创建ConfigMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E7%9B%AE%E5%BD%95%E5%88%9B%E5%BB%BAConfigMap"><span class="toc-number">26.2.4.</span> <span class="toc-text">从目录创建ConfigMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%E9%80%89%E9%A1%B9%E5%88%9B%E5%BB%BAConfigMap"><span class="toc-number">26.2.5.</span> <span class="toc-text">混合选项创建ConfigMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E7%BB%99%E5%AE%B9%E5%99%A8%E4%BC%A0%E9%80%92ConfigMap"><span class="toc-number">26.2.6.</span> <span class="toc-text">通过环境变量给容器传递ConfigMap</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-2-%E4%BD%BF%E7%94%A8Secret%E7%AE%A1%E7%90%86%E6%95%8F%E6%84%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">26.3.</span> <span class="toc-text">15.2 使用Secret管理敏感数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AEPod%E4%BD%BF%E7%94%A8Harbor%E9%95%9C%E5%83%8F"><span class="toc-number">26.3.1.</span> <span class="toc-text">配置Pod使用Harbor镜像</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Helm%E7%AE%A1%E7%90%86Kubernetes%E5%BA%94%E7%94%A8"><span class="toc-number">27.</span> <span class="toc-text">使用Helm管理Kubernetes应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Helm%E9%83%A8%E7%BD%B2"><span class="toc-number">27.1.</span> <span class="toc-text">Helm部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Helm"><span class="toc-number">27.1.1.</span> <span class="toc-text">安装Helm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Helm%E9%83%A8%E7%BD%B2%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8"><span class="toc-number">27.1.2.</span> <span class="toc-text">使用Helm部署第一个应用</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#15-2-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Helm"><span class="toc-number">28.</span> <span class="toc-text">15.2 深入理解Helm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Helm%E7%BB%84%E4%BB%B6"><span class="toc-number">28.0.1.</span> <span class="toc-text">Helm组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89Jenkins%E7%9A%84Chart"><span class="toc-number">28.0.2.</span> <span class="toc-text">自定义Jenkins的Chart</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84Chart"><span class="toc-number">28.1.</span> <span class="toc-text">创建自己的Chart</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89Nginx%E7%9A%84Chart"><span class="toc-number">28.1.1.</span> <span class="toc-text">创建自定义Nginx的Chart</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8BHelm%E5%AE%9E%E4%BE%8B"><span class="toc-number">28.1.2.</span> <span class="toc-text">查看Helm实例</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#17-%E5%BA%94%E7%94%A8%E7%9A%84%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E4%B8%8E%E5%88%86%E6%9E%90"><span class="toc-number">29.</span> <span class="toc-text">17 应用的日志采集与分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Prometheus%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-number">29.1.</span> <span class="toc-text">Prometheus快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Prometheus%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">29.1.1.</span> <span class="toc-text">Prometheus架构介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Prometheus"><span class="toc-number">29.1.2.</span> <span class="toc-text">安装Prometheus</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Node-Exporter%E9%87%87%E9%9B%86%E4%B8%BB%E6%9C%BA%E6%95%B0%E6%8D%AE"><span class="toc-number">29.1.3.</span> <span class="toc-text">使用Node Exporter采集主机数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Prometheus-UI%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE"><span class="toc-number">29.1.4.</span> <span class="toc-text">使用Prometheus UI查看数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Grafana%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">29.1.5.</span> <span class="toc-text">使用Grafana进行数据可视化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E9%83%A8%E5%88%86-Kubernetes%E9%AB%98%E7%BA%A7%E8%BF%9B%E9%98%B6"><span class="toc-number">30.</span> <span class="toc-text">第七部分 Kubernetes高级进阶</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kubernetes%E7%9A%84%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6RBAC"><span class="toc-number">31.</span> <span class="toc-text">Kubernetes的权限控制RBAC</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#22-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Pod%E8%B0%83%E5%BA%A6"><span class="toc-number">32.</span> <span class="toc-text">22 深入理解Pod调度</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Pod%E8%B0%83%E5%BA%A6"><span class="toc-number">33.</span> <span class="toc-text">深入理解Pod调度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Taints%EF%BC%88%E6%B1%A1%E7%82%B9%EF%BC%89"><span class="toc-number">33.1.</span> <span class="toc-text">Taints（污点）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B1%A1%E7%82%B9"><span class="toc-number">33.1.1.</span> <span class="toc-text">自定义污点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%B2%E7%BC%98%E6%80%A7%E8%B0%83%E5%BA%A6"><span class="toc-number">33.2.</span> <span class="toc-text">亲缘性调度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#23-Kubernetes-API%E4%BB%8B%E7%BB%8D"><span class="toc-number">34.</span> <span class="toc-text">23 Kubernetes API介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Swagger-UI%E8%BF%9B%E8%A1%8CAPI%E4%BA%A4%E4%BA%92"><span class="toc-number">34.0.1.</span> <span class="toc-text">使用Swagger UI进行API交互</span></a></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://gitee.com/xoxoyun/img/raw/master/image/0122.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">北青永恒</a></span><span id="menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://music.kococ.cn"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.2020web.cn"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">K8S(kubernetes)实践认知</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-03-09T01:16:00.000Z" title="发表于 2020-03-09 09:16:00">2020-03-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-10-20T04:18:19.426Z" title="更新于 2020-10-20 12:18:19">2020-10-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Devops/">Devops</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">20.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>88分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h1 id="Kubernetes快速入门"><a href="#Kubernetes快速入门" class="headerlink" title="Kubernetes快速入门"></a>Kubernetes快速入门</h1><p>学习Kubernetes最权威的知识来源就是Kubernetes官方文档，而且对于初学者来说，官方文档可能不是最佳选择。本章将带你循循序渐进的学习Kubernetes，后面章节会通过大量的实践案例来理解和掌握Kubernetes的知识。</p>
<ul>
<li>Kubernetes官方文档：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a></li>
<li>Kuernetes Github：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/kubernetes/">https://github.com/kubernetes/</a></li>
</ul>
<h2 id="Kubernetes架构介绍"><a href="#Kubernetes架构介绍" class="headerlink" title="Kubernetes架构介绍"></a>Kubernetes架构介绍</h2><p>Kubernetes 源于希腊语，意为 “舵手” 或 “飞行员”，是用于自动部署，扩展和管理容器化应用程序的开源系统，由于K和S之间有8个字母，被简称为K8S。Kubernetes 构建在 Google 15 年生产环境经验基础之上，可以将Kubernetes看作为Google内部的容器管理平台Brog的开源版本，当然他们之间是有一些差异的。</p>
<h3 id="Kubernetes系统架构"><a href="#Kubernetes系统架构" class="headerlink" title="Kubernetes系统架构"></a>Kubernetes系统架构</h3><p>Kubernetes被设计为Master和Node两个角色，这类似于OpenStack的架构理念，Master为控制节点，Node为计算节点或者叫工作节点，在Master节点上有一个API Server服务，对外提供标准的RestAPI，这也是Kubernetes集群的入口，意外着只要和集群进行交互必须连接到API Server上。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/4f93fc22b605a967fa54a2491557c04e.png" alt="img"></p>
<p><strong>Master节点介绍</strong></p>
<p>Kubernetes Master节点主要有4个组件，API Server、Scheduler、Controller、etcd。如下图所示：</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/0786901c59be2f756d7b979619a048b2.png" alt="img"></p>
<ul>
<li><strong>API Server</strong>：提供Kubernetes API接口，主要处理 Rest操作以及更新Etcd中的对象。是所有资源增删改查的唯一入口。</li>
<li><strong>Scheduler</strong>：绑定Pod到Node上，主要做资源调度。</li>
<li><strong>Controller Manager</strong>：所有其他群集级别的功能，目前由控制器Manager执行。资源对象的自动化控制中心，Kubernetes集群有很多控制器。</li>
<li><strong>Etcd</strong>：所有持久化的状态信息存储在Etcd中，这个是Kubernetes集群的数据库。</li>
</ul>
<p><strong>Node节点介绍</strong></p>
<p>Node节点是Kubernetes集群的工作节点，在Node节点上主要运行了Docker、Kubelet、kube-proxy三个服务（Fluentd请先忽略），如下图所示：</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/ff26ae9ea18c1a93e50b8226abfa2fa7.png" alt="img"></p>
<ul>
<li><strong>Docker Engine</strong>：负责节点的容器的管理工作，最终创建出来的是一个Docker容器。</li>
<li><strong>Kubelet</strong>：安装在Node上的代理服务，用来管理Pods以及容器、镜像、Volume等，实现对集群对节点的管理。</li>
<li><strong>Kube-proxy</strong>：安装在Node上的网络代理服务，提供网络代理以及负载均衡，实现与Service通讯。</li>
</ul>
<h3 id="Kubernetes逻辑架构"><a href="#Kubernetes逻辑架构" class="headerlink" title="Kubernetes逻辑架构"></a>Kubernetes逻辑架构</h3><p>在上面的介绍中提到像Pod、Service这些概念，在Kubernetes的系统架构图中并没有体现出来，这些可以理解为Kubernetes逻辑架构中的组件。也就是在Master和Node上并不具体存在的一个服务或者进程，但却是Kubernetes的组件，也是我们的管理对象。主要有Pod、Service和各种控制器等。</p>
<p><strong>Pod</strong></p>
<p>Pod是Kubernetes的最小管理单元，一个Pod可以包含一组容器和卷。虽然一个Pod里面可以包含一个或者多个容器，但是Pod只有一个IP地址，而且Pod重启后，IP地址会发生变化。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/e2a89d5ac819b578808e62d8fee0e960.png" alt="img"></p>
<p><strong>Controller</strong></p>
<p>一个应用如果可以有一个或者多个Pod，就像你给某一个应用做集群，集群中的所有Pod是一模一样的。Kubernetes中有很多控制器可以来管理Pod，例如下图的Replication Controller可以控制Pod的副本数量，从而实现横向扩展。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/c4ae2886ff4fdb51b9a0dd20a14c8e50.png" alt="img"></p>
<p>Kubernetes中有很多控制器，后面的章节我们会一一讲到，常用的控制器如下：</p>
<ul>
<li>Replication Controller（新版本已经被ReplicaSet所替代）</li>
<li>ReplicaSet（新版本被封装在Deployment中）</li>
<li>Deployment：封装了Pod的副本管理、部署更新、回滚、扩容、缩容等功能。</li>
<li>DaemonSet：保证所有的Node上有且只有一个Pod在运行。</li>
<li>StatefulSet：有状态的应用，为 Pod 提供唯一的标识，它可以保证部署和 scale 的顺序。</li>
<li>Job：使用Kubernetes运行单一任务。</li>
<li>CronJob：使用Kubernetes运行定时任务。</li>
</ul>
<p><strong>Service</strong></p>
<p>由于Pod的生命周期是短暂的，而且每次重启Pod的IP地址都会发生变化，而且一个Pod有多个副本，也就是说一个集群中有了多个节点，就需要考虑负载均衡的问题。Kubernetes使用Service来实现Pod的访问，而且Service有一个Cluster IP，通常也称之为VIP，是固定不变的。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/2e61328894d1eaf548ad6ff06d85a6a3.png" alt="img"></p>
<h3 id="Kubernetes网络介绍"><a href="#Kubernetes网络介绍" class="headerlink" title="Kubernetes网络介绍"></a>Kubernetes网络介绍</h3><p>在Kubernetes集群中存在着三种网络，分别是Node网络、Pod网络和Service网络，这几种网络之间的通信需要依靠网络插件，Kubernetes本身并没有提供，社区提供了像Flannel、Calico、Cannal等，后面章节会详述。</p>
<p><strong>Node网络</strong></p>
<p>Node网络指的是Kubernetes Node节点本地的网络，在本实验环境中使用的是192.168.56.0/24这个网段，所有的Node和Master在该网段都可以正常通信。</p>
<p><strong>Pod网络</strong></p>
<p>后面创建的Pod，每一个Pod都会有一个IP地址，这个IP地址网络段被称之为Pod网络，如下图所示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod -o wide</span><br><span class="line"></span><br><span class="line">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</span><br><span class="line"></span><br><span class="line">nginx-54458cd494-hpn68 1&#x2F;1 Running 0 9m7s 10.2.1.2 linux-node2.linuxhot.com</span><br><span class="line">&lt;none&gt; &lt;none&gt;</span><br><span class="line"></span><br><span class="line">nginx-54458cd494-r4mfq 1&#x2F;1 Running 0 7m46s 10.2.1.3 linux-node2.linuxhot.com</span><br><span class="line">&lt;none&gt; &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p><strong>Service网络</strong></p>
<p>Service是为Pod提供访问和负载均衡的网络地址段，如下图所示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get service</span><br><span class="line"></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line"></span><br><span class="line">kubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443&#x2F;TCP 64m</span><br><span class="line"></span><br><span class="line">nginx NodePort 10.1.216.23 &lt;none&gt; 80:30893&#x2F;TCP 8m3s</span><br></pre></td></tr></table></figure>

<p>Kubernetes的组件和知识绝非如此，快速入门可以先了解这么多，下一章节，我们先快速的部署一个Kubernetes集群。</p>
<h2 id="使用kubeadm部署Kubernetes-v1-16-4"><a href="#使用kubeadm部署Kubernetes-v1-16-4" class="headerlink" title="使用kubeadm部署Kubernetes v1.16.4"></a>使用kubeadm部署Kubernetes v1.16.4</h2><p>想要快速的体验Kubernetes的功能，官方提供了非常多的部署方案，可以使用官方提供的kubeadm以容器的方式运行Kubernetes集群，也可以使用二进制方式部署更有利于理解Kubernetes的架构，我们先使用kubeadm快速的部署一个Kubernetes集群后，学习Kubernetes的使用，然后动手使用二进制的方式来深入理解Kubernetes架构。</p>
<blockquote>
<p>注意：请不要把目光仅仅放在部署上，要慢慢的了解其本质。</p>
</blockquote>
<p>Kubernetesv1.13版本发布后，kubeadm才正式进入GA，可以生产使用。目前Kubernetes的对应镜像仓库，在国内阿里云也有了镜像站点，使用kubeadm部署Kubernetes集群变得简单并且容易了很多，本文使用kubeadm带领大家快速部署Kubernetes v1.16.2版本。</p>
<p><strong>实验环境准备</strong></p>
<p>在本书的实验环境的基础上，我们如下来分配角色：</p>
<table>
<thead>
<tr>
<th>主机名</th>
<th>IP地址（NAT）</th>
<th>最低配置</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>linux-node1.linuxhot.com</td>
<td>eth0:192.168.56.11</td>
<td>1CPU/1G内存</td>
<td>Kubernets Master/Etcd节点</td>
</tr>
<tr>
<td>linux-node2.linuxhot.com</td>
<td>eth0:192.168.56.12</td>
<td>1CPU/1G内存</td>
<td>Kubernets Node节点</td>
</tr>
<tr>
<td>linux-node3.linuxhot.com</td>
<td>eth0:192.168.56.13</td>
<td>1CPU/1G内存</td>
<td>Kubernets Node节点</td>
</tr>
<tr>
<td>Service网段</td>
<td>10.1.0.0/16</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Pod网段</td>
<td>10.2.0.0/16</td>
<td></td>
<td></td>
</tr>
<tr>
<td>备注</td>
<td>如果有条件可以部署多个Kubernets node，实验效果更佳。</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="部署Docker"><a href="#部署Docker" class="headerlink" title="部署Docker"></a>部署Docker</h3><p>首先需要在所有Kubernetes集群的节点中安装Docker和kubeadm。</p>
<p><strong>1.设置使用国内Yum源</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br><span class="line">[root@linux-node1 yum.repos.d]# wget https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br></pre></td></tr></table></figure>

<p><strong>2.安装指定的Docker版本</strong></p>
<p>由于kubeadm对Docker的版本是有要求的，需要安装与Kubernetes匹配的版本，这个对应关系一般在每次发布的Changlog中可以找到，例如1.16.2的CHANGELOG如下：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md">CHANGELOG</a></p>
<p>当前v1.16.2支持的Docker版本有v1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09,可以通过下面命令查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# yum list docker-ce.x86_64 --showduplicates | sort -r</span><br><span class="line"> * updates: mirror.jdcloud.com</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line"> * extras: mirror.jdcloud.com</span><br><span class="line"> * epel: mirrors.njupt.edu.cn</span><br><span class="line">docker-ce.x86_64            3:19.03.4-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.3-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.2-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.1-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:19.03.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.9-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.8-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.7-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.6-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.5-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.4-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.3-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.2-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.1-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            3:18.09.0-3.el7                     docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.3.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.2.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.1.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.06.0.ce-3.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.03.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            18.03.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.12.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.12.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.09.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.09.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.06.2.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.06.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.06.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.3.ce-1.el7                    docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.2.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.1.ce-1.el7.centos             docker-ce-stable</span><br><span class="line">docker-ce.x86_64            17.03.0.ce-1.el7.centos             docker-ce-stable</span><br><span class="line"> * base: mirrors.neusoft.edu.cn</span><br><span class="line">Available Packages</span><br></pre></td></tr></table></figure>

<p><strong>3.安装Docker18.09版本</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# yum -y install docker-ce-18.09.9-3.el7 docker-ce-cli-18.09.9-3.el7</span><br></pre></td></tr></table></figure>

<p><strong>4.设置cgroup驱动使用systemd</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# mkdir &#x2F;etc&#x2F;docker</span><br><span class="line">[root@linux-node1 ~]# cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;dx5z2hy7.mirror.aliyuncs.com&quot;],</span><br><span class="line">      &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p><strong>5.启动后台进程</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# systemctl enable docker &amp;&amp; systemctl start docker</span><br></pre></td></tr></table></figure>

<p><strong>6.查看Docker版本</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# docker --version</span><br><span class="line">Docker version 18.09.9, build 039a7df9ba</span><br></pre></td></tr></table></figure>

<h3 id="部署kubadm和kubelet"><a href="#部署kubadm和kubelet" class="headerlink" title="部署kubadm和kubelet"></a>部署kubadm和kubelet</h3><p>在Kubernetes集群的所有节点上部署完毕Docker后，还需要全部部署kubeadm和kubelet，其中kubeadm是管理工具，kubelet是一个服务，用于启动Kubernetes对应的服务。</p>
<p><strong>1.设置kubernetes YUM仓库</strong></p>
<p>这里在官方文档的基础上修改为了国内阿里云的yum仓库，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：最下面一行gpgkey的两个URL地址之间是空格，因为排版问题可能导致换行。</p>
</blockquote>
<p><strong>2.安装软件包</strong></p>
<p>由于版本更新频繁，请指定对应的版本号，本文采用1.16.2版本，其它版本未经测试，如果不指定版本默认安装最新版本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# yum install -y kubelet-1.16.2 kubeadm-1.16.2 kubectl-1.16.2 ipvsadm</span><br></pre></td></tr></table></figure>

<p><strong>3.配置kubelet</strong></p>
<p>默认情况下，Kubelet不允许所在的主机存在交换分区，后期规划的时候，可以考虑在系统安装的时候不创建交换分区，针对已经存在交换分区的可以设置忽略禁止使用Swap的限制，不然无法启动Kubelet。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim &#x2F;etc&#x2F;sysconfig&#x2F;kubelet</span><br><span class="line">KUBELET_CGROUP_ARGS&#x3D;&quot;--cgroup-driver&#x3D;systemd&quot;</span><br><span class="line">KUBELET_EXTRA_ARGS&#x3D;&quot;--fail-swap-on&#x3D;false&quot;</span><br></pre></td></tr></table></figure>

<p>在所有节点上关闭SWAP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# swapoff -a</span><br></pre></td></tr></table></figure>

<p><strong>4.设置内核参数</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">net.ipv4.ip_forward &#x3D; 1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<p>使配置生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# sysctl --system</span><br></pre></td></tr></table></figure>

<p><strong>5.启动kubelet并设置开机启动</strong></p>
<p>注意，此时kubelet是无法正常启动的，可以查看/var/log/messages有报错信息，等待执行初始化之后即可正常，为正常现象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure>

<p><strong>6.使用IPVS进行负载均衡</strong></p>
<p>在Kubernetes集群中Kube-Proxy组件负载均衡的功能，默认使用iptables，生产环境建议使用ipvs进行负载均衡。在所有节点启用ipvs模块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">modprobe -- ip_vs</span><br><span class="line">modprobe -- ip_vs_rr</span><br><span class="line">modprobe -- ip_vs_wrr</span><br><span class="line">modprobe -- ip_vs_sh</span><br><span class="line">modprobe -- nf_conntrack_ipv4</span><br><span class="line">[root@linux-node1 ~]# chmod +x &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules</span><br><span class="line">[root@linux-node1 ~]# source &#x2F;etc&#x2F;sysconfig&#x2F;modules&#x2F;ipvs.modules</span><br></pre></td></tr></table></figure>

<p>查看模块是否加载正常</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br><span class="line">ip_vs_sh               12688  0 </span><br><span class="line">ip_vs_wrr              12697  0 </span><br><span class="line">ip_vs_rr               12600  0 </span><br><span class="line">ip_vs                 145497  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr</span><br><span class="line">nf_conntrack_ipv4      15053  15 </span><br><span class="line">nf_defrag_ipv4         12729  1 nf_conntrack_ipv4</span><br><span class="line">nf_conntrack          133095  7 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4</span><br><span class="line">libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack</span><br></pre></td></tr></table></figure>

<ul>
<li>以上步骤请在Kubernetes的所有节点上执行，本实验环境是需要在linux-node1、linux-node2、linux-node3这三台机器上均安装Docker、kubeadm、kubelet，对于以上操作需要自动化可以参考我使用SaltStack完成的salt-kubeadm项目：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/unixhot/salt-kubeadm">https://github.com/unixhot/salt-kubeadm</a></li>
</ul>
<h3 id="初始化集群部署Master"><a href="#初始化集群部署Master" class="headerlink" title="初始化集群部署Master"></a>初始化集群部署Master</h3><p>在所有节点上安装完毕后，在linux-node1这台Master节点上进行集群的初始化工作。</p>
<p><strong>1.导出所有默认的配置</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubeadm config print init-defaults &gt; kubeadm.yaml</span><br></pre></td></tr></table></figure>

<p>上面的命令会生成一个默认配置的kubeadm配置文件，然后在此基础上进行修改即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cat kubeadm.yaml </span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">bootstrapTokens:</span><br><span class="line">- groups:</span><br><span class="line">  - system:bootstrappers:kubeadm:default-node-token</span><br><span class="line">  token: abcdef.0123456789abcdef</span><br><span class="line">  ttl: 24h0m0s</span><br><span class="line">  usages:</span><br><span class="line">  - signing</span><br><span class="line">  - authentication</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: 192.168.56.11  #修改为API Server的地址</span><br><span class="line">  bindPort: 6443</span><br><span class="line">nodeRegistration:</span><br><span class="line">  criSocket: &#x2F;var&#x2F;run&#x2F;dockershim.sock</span><br><span class="line">  name: linux-node1.example.com</span><br><span class="line">  taints:</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">---</span><br><span class="line">apiServer:</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io&#x2F;v1beta2</span><br><span class="line">certificatesDir: &#x2F;etc&#x2F;kubernetes&#x2F;pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns:</span><br><span class="line">  type: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  local:</span><br><span class="line">    dataDir: &#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">imageRepository: registry.aliyuncs.com&#x2F;google_containers  #修改为阿里云镜像仓库</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.16.2  #修改为具体的版本</span><br><span class="line">networking:</span><br><span class="line">  dnsDomain: cluster.local</span><br><span class="line">  serviceSubnet: 10.1.0.0&#x2F;16   #修改Service的网络</span><br><span class="line">  podSubnet: 10.2.0.0&#x2F;16      #新增Pod的网络</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">---   #下面有增加的三行配置，用于设置Kubeproxy使用LVS</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io&#x2F;v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: ipvs</span><br></pre></td></tr></table></figure>

<p><strong>2. 执行初始化操作</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubeadm init --config kubeadm.yaml</span><br><span class="line">[init] Using Kubernetes version: v1.16.2</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">error execution phase preflight: [preflight] Some fatal errors occurred:</span><br><span class="line">        [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2</span><br><span class="line">        [ERROR Swap]: running with swap on is not supported. Please disable swa</span><br><span class="line">[preflight] If you know what you are doing, you can make a check non-fatal with &#96;--ignore-preflight-errors&#x3D;...&#96;</span><br><span class="line">To see the stack trace of this error execute with --v&#x3D;5 or higher</span><br></pre></td></tr></table></figure>

<p>如果遇到上面这样的报错，是因为在实验环境开启了交换分区，以及CPU的核数小于2造成的，可以使用–ignore-preflight-errors=进行忽略。 –ignore-preflight-errors=：忽略运行时的错误，例如上面目前存在[ERROR NumCPU]和[ERROR Swap]，忽略这两个报错就是增加–ignore-preflight-errors=NumCPU 和–ignore-preflight-errors=Swap的配置即可。</p>
<p>再次执行初始化操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubeadm init --config kubeadm.yaml \</span><br><span class="line">  --ignore-preflight-errors&#x3D;Swap,NumCPU </span><br><span class="line">[init] Using Kubernetes version: v1.16.2</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">        [WARNING NumCPU]: the number of available CPUs 1 is less than the required 2</span><br><span class="line">        [WARNING Swap]: running with swap on is not supported. Please disable swap</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;</span><br></pre></td></tr></table></figure>

<p>执行完毕后，会在当前输出下停留，等待下载Kubernetes组件的Docker镜像。根据你的网络情况，可以持续1-5分钟，你也可以使用docker images查看下载的镜像。镜像下载完毕之后，就会进行初始操作：</p>
<p>这里省略了所有输出，初始化操作主要经历了下面15个步骤，每个阶段均输出均使用[步骤名称]作为开头：</p>
<ol>
<li>[init]：指定版本进行初始化操作</li>
<li>[preflight] ：初始化前的检查和下载所需要的Docker镜像文件。</li>
<li>[kubelet-start]：生成kubelet的配置文件”/var/lib/kubelet/config.yaml”，没有这个文件kubelet无法启动，所以初始化之前的kubelet实际上启动失败。</li>
<li>[certificates]：生成Kubernetes使用的证书，存放在/etc/kubernetes/pki目录中。</li>
<li>[kubeconfig] ：生成 KubeConfig文件，存放在/etc/kubernetes目录中，组件之间通信需要使用对应文件。</li>
<li>[control-plane]：使用/etc/kubernetes/manifest目录下的YAML文件，安装 Master组件。</li>
<li>[etcd]：使用/etc/kubernetes/manifest/etcd.yaml安装Etcd服务。</li>
<li>[wait-control-plane]：等待control-plan部署的Master组件启动。</li>
<li>[apiclient]：检查Master组件服务状态。</li>
<li>[uploadconfig]：更新配置</li>
<li>[kubelet]：使用configMap配置kubelet。</li>
<li>[patchnode]：更新CNI信息到Node上，通过注释的方式记录。</li>
<li>[mark-control-plane]：为当前节点打标签，打了角色Master，和不可调度标签，这样默认就不会使用Master节点来运行Pod。</li>
<li>[bootstrap-token]：生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到</li>
<li>[addons]：安装附加组件CoreDNS和kube-proxy</li>
</ol>
<p>成功执行之后，你会看到下面的输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line">kubeadm join 192.168.56.11:6443 --token 19fhhl.3mzkyk16tcgp6vga --discovery-token-ca-cert-hash sha256:76a88c38b673d3b2ac73e33127a809688cb3e58c533512ac6d92ecb66aa57a45</span><br></pre></td></tr></table></figure>

<p>如果执行失败，那意味着之前的操作存在问题，检查顺序如下：</p>
<ul>
<li>基础环境</li>
<li>主机名是否可以解析，SELinux，iptables是否关闭。</li>
<li>交换分区是否存在free -m查看</li>
<li>内核参数是否修改、IPVS是否修改（目前阶段不会造成失败）</li>
<li>基础软件</li>
<li>Docker是否安装并启动</li>
<li>Kubelet是否安装并启动</li>
<li>执行kubeadm是否有别的报错是否忽略</li>
<li>systemctl status kubelet查看kubelet是否启动</li>
<li>如果kubelet无法启动，查看日志有什么报错，并解决报错。</li>
<li>以上都解决完毕，需要重新初始化</li>
<li>kubeadm reset 进行重置（生产千万不要执行，会直接删除集群）</li>
<li>根据kubeadm reset 提升，清楚iptables和LVS。</li>
</ul>
<p>请根据上面输出的要求配置kubectl命令来访问集群。</p>
<p><strong>3.为kubectl准备Kubeconfig文件。</strong></p>
<p>kubectl默认会在执行的用户家目录下面的.kube目录下寻找config文件。这里是将在初始化时[kubeconfig]步骤生成的admin.conf拷贝到.kube/config。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# mkdir -p $HOME&#x2F;.kube</span><br><span class="line">[root@linux-node1 ~]# cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">[root@linux-node1 ~]# chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure>

<p>在该配置文件中，记录了API Server的访问地址，所以后面直接执行kubectl命令就可以正常连接到API Server中。</p>
<p><strong>4.使用kubectl命令查看组件状态</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get cs</span><br><span class="line">NAME STATUS MESSAGE ERROR</span><br><span class="line">scheduler Healthy ok</span><br><span class="line">controller-manager Healthy ok</span><br><span class="line">etcd-0 Healthy &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>**知识回顾：为什么上面的输出没有显示API Server组件的状态</p>
<p>因为API Server是Kubernetes集群的入口，所有和Kubernetes集群的交互都必须经过APIServer，kubectl命令也是连接到API Server上进行交互，所以如果能够正常使用kubectl执行命令，意味着API Server运行正常。</p>
<p><strong>5.使用kubectl获取Node信息</strong></p>
<p>目前只有一个节点，角色是Master，状态是NotReady。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get node</span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">linux-node1.unixhot.com NotReady master 14m v1.16.2</span><br></pre></td></tr></table></figure>

<h3 id="部署网络插件"><a href="#部署网络插件" class="headerlink" title="部署网络插件"></a>部署网络插件</h3><p>Master节点NotReady的原因就是因为没有使用任何的网络插件，此时Node和Master的连接还不正常。目前最流行的Kubernetes网络插件有Flannel、Calico、Canal，这里分别列举了Canal和Flannel，你可以选择其中之一进行部署。 因为基础的Kubernetes集群已经配置完毕，后面的增加组件等操作，几乎都可以使用kubectl和一个YAML配置文件来完成。</p>
<p>【部署canal网络插件】</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl apply -f https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.3&#x2F;getting-started&#x2F;kubernetes&#x2F;installation&#x2F;hosted&#x2F;canal&#x2F;rbac.yaml</span><br><span class="line">[root@linux-node1 ~]# kubectl apply -f https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;v3.3&#x2F;getting-started&#x2F;kubernetes&#x2F;installation&#x2F;hosted&#x2F;canal&#x2F;canal.yaml</span><br></pre></td></tr></table></figure>

<p>【部署Flannel网络插件】（推荐） 部署Flannel网络插件需要修改Pod的IP地址段，修改为和你初始化一直的网段，可以先下载Flannel的YAML文件修改后，再执行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# git clone --depth 1 https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;flannel.git</span><br><span class="line">[root@linux-node1 ~]# cd flannel&#x2F;Documentation&#x2F;</span><br><span class="line">[root@linux-node1 Documentation]# vim kube-flannel.yml</span><br><span class="line"># 修改&quot;Network&quot;: &quot;10.244.0.0&#x2F;16&quot;为&quot;Network&quot;: &quot;10.2.0.0&#x2F;16&quot;,</span><br><span class="line"></span><br><span class="line">74   net-conf.json: |</span><br><span class="line">75     &#123;</span><br><span class="line">76       &quot;Network&quot;: &quot;10.2.0.0&#x2F;16&quot;,</span><br><span class="line">77       &quot;Backend&quot;: &#123;</span><br><span class="line">78         &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">79       &#125;</span><br><span class="line">80     &#125;</span><br><span class="line"></span><br><span class="line"># 请注意，Flannel的镜像拉取速度会比较慢，可以替换为国内镜像</span><br><span class="line"># image: quay.io&#x2F;coreos&#x2F;flannel:v0.10.0-amd64</span><br><span class="line">image: quay-mirror.qiniu.com&#x2F;coreos&#x2F;flannel:v0.11.0-amd64</span><br><span class="line"></span><br><span class="line"># 如果Node中有多个网卡，可以使用--iface来指定对应的网卡参数。</span><br><span class="line">containers:</span><br><span class="line">      - name: kube-flannel</span><br><span class="line">        image: quay-mirror.qiniu.com&#x2F;coreos&#x2F;flannel:v0.11.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - &#x2F;opt&#x2F;bin&#x2F;flanneld</span><br><span class="line">        args:</span><br><span class="line">        - --ip-masq</span><br><span class="line">        - --kube-subnet-mgr</span><br><span class="line">        - --iface&#x3D;eth0</span><br></pre></td></tr></table></figure>

<p>部署Flannel</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 Documentation]# kubectl create -f kube-flannel.yml</span><br></pre></td></tr></table></figure>

<p>查看Pod状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 Documentation]# kubectl get pod -n kube-system</span><br><span class="line">NAME                                              READY   STATUS     RESTARTS   AGE</span><br><span class="line">coredns-58cc8c89f4-cjhdv                          0&#x2F;1     Pending    0          41m</span><br><span class="line">coredns-58cc8c89f4-vdfn2                          0&#x2F;1     Pending    0          41m</span><br><span class="line">etcd-linux-node1.unixhot.com                      1&#x2F;1     Running    0          41m</span><br><span class="line">kube-apiserver-linux-node1.unixhot.com            1&#x2F;1     Running    0          40m</span><br><span class="line">kube-controller-manager-linux-node1.unixhot.com   1&#x2F;1     Running    1          40m</span><br><span class="line">kube-flannel-ds-amd64-bwsxl                       0&#x2F;1     Init:0&#x2F;1   0          20s</span><br><span class="line">kube-proxy-5qrmh                                  1&#x2F;1     Running    0          41m</span><br><span class="line">kube-scheduler-linux-node1.unixhot.com            1&#x2F;1     Running    1          41m</span><br></pre></td></tr></table></figure>

<p>可以看到此时CoreDNS处于Pending状态，需要等待网络插件canal或者Flannel的Pod状态变成Running之后CoreDNS也会正常。所有Pod的状态都变成Running之后，这个时候再次获取Node，会发现节点变成了Ready状态。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get node</span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">linux-node1.unixhot.com Ready master 29m v1.16.2</span><br></pre></td></tr></table></figure>

<p><em>kubeadm其实使用Kubernetes部署Kubernetes，这样就存在先有鸡还是先有蛋的问题，所以，我们首先手动部署了Docker和kubelet，然后kubeadm调用kubelet以静态Pod的方式部署了Kubernetes集群中的其它组件。静态Pod在后面的章节会讲到。</em></p>
<h3 id="部署Node节点"><a href="#部署Node节点" class="headerlink" title="部署Node节点"></a>部署Node节点</h3><p>Master节点部署完毕之后，就可以部署Node节点，首先请遵循部署Docker和kubeadm章节为Node节点部署安装好docker、kubeadm和kubelet，此过程这里不再重复列出。</p>
<p><strong>1.在Master节点输出增加节点的命令</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubeadm token create --print-join-command</span><br><span class="line">kubeadm join 192.168.56.11:6443 --token isggqa.xjwsm3i6nex91d2x --discovery-token-ca-cert-hash sha256:718827895a9a5e63dfa9ff54e16ad6dc0c493139c9c573b67ad66968036cd569</span><br></pre></td></tr></table></figure>

<p><strong>2.在Node节点执行</strong></p>
<p>注意如果节点有交换分区，需要增加–ignore-preflight-errors=Swap。</p>
<p>部署linux-node2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node2 ~]# kubeadm join 192.168.56.11:6443 --token isggqa.xjwsm3i6nex91d2x --discovery-token-ca-cert-hash sha256:718827895a9a5e63dfa9ff54e16ad6dc0c493139c9c573b67ad66968036cd569 --ignore-preflight-errors&#x3D;Swap</span><br></pre></td></tr></table></figure>

<p>部署linux-node3</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node3 ~]# kubeadm join 192.168.56.11:6443 --tokenisggqa.xjwsm3i6nex91d2x --discovery-token-ca-cert-hash sha256:718827895a9a5e63dfa9ff54e16ad6dc0c493139c9c573b67ad66968036cd569 --ignore-preflight-errors&#x3D;Swap</span><br></pre></td></tr></table></figure>

<p>这个时候kubernetes会使用DaemonSet在所有节点上都部署canal/flannel和kube-proxy。部署完毕之后节点即部署完毕。DaemonSet的内容后面会讲解。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get daemonset --all-namespaces</span><br><span class="line">NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE</span><br><span class="line">kube-system canal 2 2 1 2 1 beta.kubernetes.io&#x2F;os&#x3D;linux 17m</span><br><span class="line">kube-system kube-proxy 2 2 2 2 2 &lt;none&gt; 47m</span><br></pre></td></tr></table></figure>

<p>待所有Pod全部启动完毕之后，节点就恢复Ready状态。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod --all-namespaces</span><br><span class="line">NAMESPACE NAME READY STATUS RESTARTS AGE</span><br><span class="line">kube-system canal-lv92w 3&#x2F;3 Running 0 8m45s</span><br><span class="line">kube-system canal-rq5n5 3&#x2F;3 Running 0 23m</span><br><span class="line">kube-system coredns-78d4cf999f-5k4sg 1&#x2F;1 Running 0 53m</span><br><span class="line">kube-system coredns-78d4cf999f-bnbgf 1&#x2F;1 Running 0 53m</span><br><span class="line">kube-system etcd-linux-node1.linuxhot.com 1&#x2F;1 Running 0 52m</span><br><span class="line">kube-system kube-apiserver-linux-node1.linuxhot.com 1&#x2F;1 Running 0 52m</span><br><span class="line">kube-system kube-controller-manager-linux-node1.linuxhot.com 1&#x2F;1 Running 0 52m</span><br><span class="line">kube-system kube-proxy-sddlp 1&#x2F;1 Running 0 53m</span><br><span class="line">kube-system kube-proxy-tw96b 1&#x2F;1 Running 0 8m45s</span><br><span class="line">kube-system kube-scheduler-linux-node1.linuxhot.com 1&#x2F;1 Running 0 52m</span><br></pre></td></tr></table></figure>

<p><strong>查看所有节点</strong> `` [root@linux-node1 ~]# kubectl get node NAME STATUS ROLES AGE VERSION linux-node1.linuxhot.com Ready master 49m v1.13.2 linux-node2.linuxhot.com Ready 4m48s v1.13.2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">**如何给Node加上Roles标签**</span><br><span class="line"></span><br><span class="line">使用kubectl get node能够看到linux-node1.linuxhot.com的ROLES是master这个是在进行集群初始化的时候[mark-control-plane]进行标记的。</span><br><span class="line">[mark-control-plane] Marking the node linux-node1.linuxhot.com as control-plane</span><br><span class="line">by adding the label &quot;node-role.kubernetes.io&#x2F;master&#x3D;&#39;&#39;&quot;</span><br><span class="line"></span><br><span class="line">[mark-control-plane] Marking the node linux-node1.linuxhot.com as control-plane</span><br><span class="line">by adding the taints [node-role.kubernetes.io&#x2F;master:NoSchedule]</span><br><span class="line">1.查看节点的标签</span><br><span class="line">[root@linux-node1 ~]# kubectl get nodes --show-labels</span><br><span class="line"></span><br><span class="line">NAME STATUS ROLES AGE VERSION LABELS</span><br><span class="line"></span><br><span class="line">linux-node1.linuxhot.com Ready master 48m v1.13.3</span><br><span class="line">beta.kubernetes.io&#x2F;arch&#x3D;amd64,beta.kubernetes.io&#x2F;os&#x3D;linux,kubernetes.io&#x2F;hostname&#x3D;linux-node1.linuxhot.com,node-role.kubernetes.io&#x2F;master&#x3D;</span><br><span class="line"></span><br><span class="line">linux-node2.linuxhot.com Ready &lt;none&gt; 7m13s v1.13.3</span><br><span class="line">beta.kubernetes.io&#x2F;arch&#x3D;amd64,beta.kubernetes.io&#x2F;os&#x3D;linux,kubernetes.io&#x2F;hostname&#x3D;linux-node2.linuxhot.com</span><br><span class="line">2.增加标签</span><br><span class="line">[root@linux-node1 ~]# kubectl label nodes linux-node2.linuxhot.com</span><br><span class="line">node-role.kubernetes.io&#x2F;node&#x3D;</span><br><span class="line"></span><br><span class="line">node&#x2F;linux-node2.linuxhot.com labeled</span><br><span class="line">3.查看效果</span><br><span class="line">[root@linux-node1 ~]# kubectl get nodes</span><br><span class="line">NAME STATUS ROLES AGE VERSION</span><br><span class="line">linux-node1.linuxhot.com Ready master 50m v1.13.3</span><br><span class="line">linux-node2.linuxhot.com Ready node 8m41s v1.13.3</span><br><span class="line">### 测试Kubernetes集群 &#123;#test&#125;</span><br><span class="line"></span><br><span class="line">在上面的步骤中，我们创建了一个Kubernetes集群，1个Master和2个Node节点，在生产环境需要考虑Master的高可用，这里先不用考虑，后面会讲到。</span><br><span class="line"></span><br><span class="line">**1.创建一个单Pod的Nginx应用**</span><br><span class="line">[root@linux-node1 ~]# kubectl create deployment nginx --image&#x3D;nginx:alpine</span><br><span class="line">deployment.apps&#x2F;nginx created</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl get pod</span><br><span class="line">NAME READY STATUS RESTARTS AGE</span><br><span class="line">nginx-54458cd494-9j7ql 0&#x2F;1 ContainerCreating 0 10s</span><br><span class="line">**2.查看Pod详细信息**</span><br><span class="line"></span><br><span class="line">待Pod的状态为Running后，可以获取Pod的IP地址，这个IP地址是从Master节点初始化的--pod-network-cidr&#x3D;10.2.0.0&#x2F;16地址段中分配的。</span><br><span class="line">[root@linux-node1 ~]# kubectl get pod -o wide</span><br><span class="line"></span><br><span class="line">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES</span><br><span class="line"></span><br><span class="line">nginx-54458cd494-9j7ql 1&#x2F;1 Running 0 59s 10.2.1.2 linux-node2.linuxhot.com</span><br><span class="line">&lt;none&gt; &lt;none&gt;</span><br><span class="line">**3.测试Nginx访问**</span><br><span class="line">[root@linux-node1 ~]# curl --head http:&#x2F;&#x2F;10.2.1.2</span><br><span class="line"></span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.15.8</span><br><span class="line">Date: Sun, 13 Jan 2019 01:16:36 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 612</span><br><span class="line">Last-Modified: Wed, 26 Dec 2018 23:21:49 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">ETag: &quot;5c240d0d-264&quot;</span><br><span class="line">Accept-Ranges: bytes</span><br><span class="line">**4.测试扩容**</span><br><span class="line"></span><br><span class="line">现在将Nginx应用的Pod副本数量拓展到2个节点</span><br><span class="line">[root@linux-node1 ~]# kubectl scale deployment nginx --replicas&#x3D;2</span><br><span class="line"></span><br><span class="line">deployment.extensions&#x2F;nginx scaled</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl get pod</span><br><span class="line"></span><br><span class="line">NAME READY STATUS RESTARTS AGE</span><br><span class="line"></span><br><span class="line">nginx-54458cd494-9j7ql 1&#x2F;1 Running 0 2m13s</span><br><span class="line"></span><br><span class="line">nginx-54458cd494-vnm4f 1&#x2F;1 Running 0 5s</span><br><span class="line">**5.为Nginx增加Service**</span><br><span class="line"></span><br><span class="line">为Nginx增加Service，会创建一个Cluster</span><br><span class="line">IP，从Master初始化的--service-cidr&#x3D;10.1.0.0&#x2F;16地址段中进行分配，</span><br><span class="line">并开启NodePort是在Node节点上进行端口映射，进行外部访问。</span><br><span class="line">[root@linux-node1 ~]# kubectl expose deployment nginx --port&#x3D;80</span><br><span class="line">--type&#x3D;NodePort</span><br><span class="line"></span><br><span class="line">service&#x2F;nginx exposed</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl get service</span><br><span class="line"></span><br><span class="line">NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</span><br><span class="line">kubernetes ClusterIP 10.1.0.1 &lt;none&gt; 443&#x2F;TCP 88m</span><br><span class="line">nginx NodePort 10.1.147.204 &lt;none&gt; 80:30599&#x2F;TCP 67m</span><br><span class="line">**6.测试Service的VIP**</span><br><span class="line">[root@linux-node1 ~]# curl --head http:&#x2F;&#x2F;10.1.147.204&#x2F;</span><br><span class="line"></span><br><span class="line">HTTP&#x2F;1.1 200 OK </span><br><span class="line">Server: nginx&#x2F;1.15.8</span><br><span class="line">Date: Sun, 13 Jan 2019 01:26:21 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 612</span><br><span class="line">Last-Modified: Wed, 26 Dec 2018 23:21:49 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">ETag: &quot;5c240d0d-264&quot;</span><br><span class="line">Accept-Ranges: bytes</span><br></pre></td></tr></table></figure>

<p>``` <strong>7.测试NodePort，外部访问。</strong></p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/1f9d523f359ce6d49515d04703d8e941.png" alt="img"></p>
<p>这一切看起来似乎不是十分完美，但是现在你已经拥有了一个Kubernetes集群，接下来就可以继续探索Kubernetes的世界了。</p>
<h2 id="公有云中的Kubernetes"><a href="#公有云中的Kubernetes" class="headerlink" title="公有云中的Kubernetes"></a>公有云中的Kubernetes</h2><p>截止2019年2月，大多数公有云已经提供了容器Kubernetes的产品服务，对于使用公有云的用户来说，最佳实践是直接购买公有云产品，而非自己部署Kubernetes集群，主要是因为公有云已经将网络和存储与Kubernetes集成好了，解决了生产应用的难题。</p>
<h3 id="阿里云中的Kubernetes"><a href="#阿里云中的Kubernetes" class="headerlink" title="阿里云中的Kubernetes"></a>阿里云中的Kubernetes</h3><p>国内阿里云提供了容器服务 Kubernetes 版（简称 ACK）提供高性能可伸缩的容器应用管理能力，支持企业级 Kubernetes 容器化应用的全生命周期管理。容器服务 Kubernetes 版简化集群的搭建和扩容等工作，整合阿里云虚拟化、存储、网络和安全能力，打造云端最佳的 Kubernetes 容器化应用运行环境。</p>
<p><strong>阿里云Kubernetes模式</strong></p>
<p>容器服务Kubernetes版包含了经典Dedicated Kubernetes以及Serverless两种形态，方便您按需选择。</p>
<ul>
<li>经典Dedicated Kubernetes模式：您可以对集群基础设施和容器应用进行更细粒度的控制，比如选择宿主机实例规格和操作系统，指定Kubernetes 版本、自定义 Kubernetes 特性开关设置等。阿里云 Kubernetes 服务负责为集群创建底层云资源，升级等自动化运维操作。而您需要规划、维护、升级服务器集群，手动或自动在集群中添加或删除服务器。</li>
<li>Serverless 模式：您无需创建底层虚拟化资源，可以利用 Kubernetes 命令指明应用容器镜像、CPU 和内存要求以及对外服务方式，直接启动。</li>
</ul>
<p><strong>阿里云Kubernetes产品架构</strong></p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/43688d33a81f2bd6af354d0715b6b297.png" alt="C:\\Users\\jason\\Desktop\\15447553537457_zh-CN.png"></p>
<p><strong>阿里云Kubernetes创建</strong></p>
<p>默认情况下可以在阿里云中自行创建5个集群，每个集群最多可以添加 40 个节点。如需更高配额，需要提交工单申请。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/4c165d0ebc842862595f9fd0f4650d43.png" alt="img"></p>
<p>阿里云将网络、存储、安全等方面已经进行了深度集成，在创建Kubernetes集群时，阿里云容器服务提供两种网络插件：Terway和Flannel：</p>
<ul>
<li>Flannel：使用的是简单稳定的社区的Flannel CNI插件，配合阿里云的VPC的高速网络，能给集群高性能和稳定的容器网络体验，但功能偏简单，支持的特性少，例如：不支持基于Kubernetes标准的Network Policy。</li>
<li>Terway：是阿里云容器服务自研的网络插件，功能上完全兼容Flannel，支持将阿里云的弹性网卡分配给容器，支持基于Kubernetes标准的NetworkPolicy来定义容器间的访问策略，支持对单个容器做带宽的限流。对于不需要使用Network Policy的用户，可以选择Flannel，其他情况建议选择Terway。了解更多Terway网络插件的相关内容，请参见Terway网络插件。</li>
</ul>
<p>最终阿里云会使用kubeadm帮你创建一个指定版本的Kubernetes集群。</p>
<h3 id="私有云中的Kubernetes"><a href="#私有云中的Kubernetes" class="headerlink" title="私有云中的Kubernetes"></a>私有云中的Kubernetes</h3><p><strong>内网部署Kubernetes</strong></p>
<p>很多企业需要内网部署Kubernetes但是内网又无法访问外网，就需要本地化部署，无忘了本地话部署主要包括两个方面，一个是软件仓库的准备，一个是Kubernetes镜像的准备。</p>
<ol>
<li>准备内网YUM仓库（略）</li>
<li>准备Docker Registry（请参考Harbor章节）</li>
<li>下载并提交镜像到Harbor中，然后将Harbor的镜像部署直接COPY到生产环境中</li>
</ol>
<p>下载脚本如下，请根据需求自行修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim k8s-images.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line"># EVN</span><br><span class="line">ALIYUN_REG&#x3D;&quot;registry.aliyuncs.com&#x2F;google_containers&quot;</span><br><span class="line">FLANNEL_REG&#x3D;&quot;quay-mirror.qiniu.com&#x2F;coreos&quot;</span><br><span class="line">LOCAL_REG&#x3D;&quot;192.168.56.11&#x2F;kubernetes&quot;</span><br><span class="line">K8S_VER&#x3D;v1.15.5</span><br><span class="line">PAUSE_VER&#x3D;3.1</span><br><span class="line">ETCD_VER&#x3D;3.3.10</span><br><span class="line">COREDNS_VER&#x3D;1.3.1</span><br><span class="line">FLANNEL_VER&#x3D;v0.11.0-amd64</span><br><span class="line"></span><br><span class="line"># Kubernetes Docker Images</span><br><span class="line">IMAGES&#x3D;(</span><br><span class="line">  kube-proxy:$&#123;K8S_VER&#125;</span><br><span class="line">  kube-scheduler:$&#123;K8S_VER&#125;</span><br><span class="line">  kube-controller-manager:$&#123;K8S_VER&#125;</span><br><span class="line">  kube-apiserver:$&#123;K8S_VER&#125;</span><br><span class="line">  pause:$&#123;PAUSE_VER&#125;</span><br><span class="line">  etcd-amd64:$&#123;ETCD_VER&#125;</span><br><span class="line">  coredns:$&#123;COREDNS_VER&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">for IMAGE in $&#123;IMAGES[@]&#125;</span><br><span class="line">do</span><br><span class="line">  docker pull $&#123;ALIYUN_REG&#125;&#x2F;$&#123;IMAGE&#125;</span><br><span class="line">  docker tag $&#123;ALIYUN_REG&#125;&#x2F;$&#123;IMAGE&#125; $&#123;LOCAL_REG&#125;&#x2F;$&#123;IMAGE&#125;</span><br><span class="line">  #docker push $&#123;LOCAL_REG&#125;&#x2F;$&#123;IMAGES&#125;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"># Flannel</span><br><span class="line"></span><br><span class="line">docker pull $&#123;FLANNEL_REG&#125;&#x2F;flannel:$&#123;FLANNEL_VER&#125;</span><br><span class="line">docker tag $&#123;FLANNEL_REG&#125;&#x2F;flannel:$&#123;FLANNEL_VER&#125; $&#123;LOCAL_REG&#125;&#x2F;flannel:$&#123;FLANNEL_VER&#125;</span><br></pre></td></tr></table></figure>

<h1 id="第五部分-将应用迁移至Kubernetes"><a href="#第五部分-将应用迁移至Kubernetes" class="headerlink" title="第五部分 将应用迁移至Kubernetes"></a>第五部分 将应用迁移至Kubernetes</h1><h1 id="6-第一步：将应用封装进容器中"><a href="#6-第一步：将应用封装进容器中" class="headerlink" title="6 第一步：将应用封装进容器中"></a>6 第一步：将应用封装进容器中</h1><h2 id="第一步：将应用封装进容器中"><a href="#第一步：将应用封装进容器中" class="headerlink" title="第一步：将应用封装进容器中"></a>第一步：将应用封装进容器中</h2><p>在之前的容器镜像实战中，我们已经学习了如何将应用容器化，这里我们将下载使用两个官方的Nginx镜像来完成接下来的实验。</p>
<h3 id="部署Harbor镜像仓库"><a href="#部署Harbor镜像仓库" class="headerlink" title="部署Harbor镜像仓库"></a>部署Harbor镜像仓库</h3><p>生产环境中可以使用Harbor来管理Docker镜像，请参考之前章节的内容完成Harbor镜像仓库的部署工作，并在Harbor中创建一个devopsedu的项目。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/093b4b731c05b54a1b8426e924f93d45.png" alt="img"></p>
<h3 id="制作实验用的Docker镜像"><a href="#制作实验用的Docker镜像" class="headerlink" title="制作实验用的Docker镜像"></a>制作实验用的Docker镜像</h3><p>这里不再演示Docker镜像的构建，直接下载两个官方镜像作为案例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# docker pull nginx:1.13.12</span><br><span class="line">[root@linux-node1 ~]# docker pull nginx:1.14.0</span><br></pre></td></tr></table></figure>

<h3 id="配置Docker仓库"><a href="#配置Docker仓库" class="headerlink" title="配置Docker仓库"></a>配置Docker仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root\@linux-node1 ~]# vim &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;tdimi5q1.mirror.aliyuncs.com&quot;],</span><br><span class="line">    &quot;insecure-registries&quot; : [&quot;http:&#x2F;&#x2F;192.168.56.11&quot;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# systemctl restart docker</span><br></pre></td></tr></table></figure>

<h3 id="登录Harbor镜像仓库"><a href="#登录Harbor镜像仓库" class="headerlink" title="登录Harbor镜像仓库"></a>登录Harbor镜像仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# docker login 192.168.56.11</span><br><span class="line">Username: admin</span><br><span class="line">Password:</span><br><span class="line">WARNING! Your password will be stored unencrypted in &#x2F;root&#x2F;.docker&#x2F;config.json.</span><br><span class="line">    Configure a credential helper to remove this warning. See</span><br><span class="line">    https:&#x2F;&#x2F;docs.docker.com&#x2F;engine&#x2F;reference&#x2F;commandline&#x2F;login&#x2F;\#credentials-store</span><br><span class="line">    Login Succeeded</span><br></pre></td></tr></table></figure>

<h3 id="提交镜像到Registry"><a href="#提交镜像到Registry" class="headerlink" title="提交镜像到Registry"></a>提交镜像到Registry</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# docker tag nginx:1.13.12</span><br><span class="line">192.168.56.11&#x2F;devopsedu&#x2F;nginx:1.13.12</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# docker tag nginx:1.14.0</span><br><span class="line">192.168.56.11&#x2F;devopsedu&#x2F;nginx:1.14.0</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# docker push 192.168.56.11&#x2F;devopsedu&#x2F;nginx:1.13.12</span><br><span class="line">[root@linux-node1 ~]# docker push 192.168.56.11&#x2F;devopsedu&#x2F;nginx:1.14.0</span><br></pre></td></tr></table></figure>

<p>在上面的步骤中，模拟了生产环境如何构建和提交镜像，如果对于构建和提交镜像有疑问可以复习第3章的内容。</p>
<h1 id="7-第二步：将容器封装到Pod中"><a href="#7-第二步：将容器封装到Pod中" class="headerlink" title="7 第二步：将容器封装到Pod中"></a>7 第二步：将容器封装到Pod中</h1><p>Pod是Kubernetes最小的管理单元，一个Pod可以代表一个运行在集群里的进程。之前是在宿主机上运行不同的进程，现在是运行不同的Pod。前面介绍过Pod是一个逻辑架构的组件，Pod里封装了一个（或者多个）应用容器，存储资源和IP地址。</p>
<p><strong>为什么要造一个Pod出来？</strong> 学习Kubernetes遇到的最多的名称可能就是Pod了，其它开源的容器管理平台例如Mesos直接管理和调度的是容器，但是Kubernetes确是Pod，它在容器上面做了一层封装，方便用户将一组紧耦合的容器，放置在一个共享资源的单元中。对于很多没有此类场景的初学者，可以暂时将Pod看做是容器的一个壳，你也完全可以只在Pod中运行一个容器，随着学习的深入再慢慢理解。 Kubernetes运行Pod的两种方式：</p>
<ul>
<li>Pod里只运行一个单独容器，是Kubernetes最常见的使用场景；在这种情况下，可以把Pod看做是一个单独容器的连接器，Kubernetes通过Pod去管理容器，作为使用者几乎不用关心容器。</li>
<li>Pod里运行多个有关系容器。例如如果使用Nginx+Tomcat运行Java应用，可以制作一个镜像里面包含了Nginx+Tomcat，也可以分别制作两个镜像Nginx镜像和Tomcat镜像，如果使用Kubernetes就需要使用Pod，如果将Nginx和Tomcat单独放在两个Pod里面来管理，就会面临很多很多问题。这个时候，就可以把这两个容器放置在一个Pod中。</li>
</ul>
<h2 id="Pod管理"><a href="#Pod管理" class="headerlink" title="Pod管理"></a>Pod管理</h2><p>在Kubernetes中使用YAML格式来描述一个Pod。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim nginx-pod.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx:1.13.12</span><br><span class="line">    ports:</span><br><span class="line">- containerPort: 80</span><br></pre></td></tr></table></figure>

<p>Pod的YAML描述内容还有很多，在使用kubeadm部署Kubernetes的时候，就是使用静态Pod的方式运行的相关服务，YAML文件存放在，当然现在很多配置还是看不懂的，带着问题继续学习。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# ls -l &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;</span><br><span class="line">total 16</span><br><span class="line">-rw------- 1 root root 2041 Feb 11 20:33 etcd.yaml</span><br><span class="line">-rw------- 1 root root 2700 Feb 11 20:33 kube-apiserver.yaml</span><br><span class="line">-rw------- 1 root root 2345 Feb 11 20:33 kube-controller-manager.yaml</span><br><span class="line">-rw------- 1 root root 1080 Feb 11 20:33 kube-scheduler.yaml</span><br></pre></td></tr></table></figure>

<p>创建Pod</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl create -f nginx-pod.yaml </span><br><span class="line">pod &quot;nginx-pod&quot; created</span><br></pre></td></tr></table></figure>

<p>查看Pod</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-pod                           1&#x2F;1       Running   0          49s</span><br></pre></td></tr></table></figure>

<p>查看Pod更多信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod -o wide</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">nginx-pod  1&#x2F;1       Running   0          1m        10.2.53.18   192.168.56.13</span><br></pre></td></tr></table></figure>

<p>查看Pod详情</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl describe pod nginx-pod</span><br><span class="line">Name:         nginx-pod</span><br><span class="line">Namespace:    default</span><br><span class="line">Node:         192.168.56.13&#x2F;192.168.56.13</span><br><span class="line">Start Time:   Sat, 02 Jun 2018 06:42:53 +0800</span><br><span class="line">Labels:       app&#x3D;nginx</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           10.2.53.18</span><br></pre></td></tr></table></figure>

<p>查看Pod日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl logs pod&#x2F;nginx-pod</span><br></pre></td></tr></table></figure>

<p><strong>Pod中的镜像拉取策略</strong> 当kubelet尝试拉取指定的镜像时，[imagePullPolicy]和镜像的标签会生效。</p>
<ul>
<li>imagePullPolicy: IfNotPresent：仅当镜像在本地不存在时镜像才被拉取。</li>
<li>imagePullPolicy: Always：每次启动 pod 的时候都会拉取镜像。</li>
</ul>
<p>省略imagePullPolicy，镜像标签为:latest或被省略，Always被应用。 imagePullPolicy被省略，并且镜像的标签被指定且不是:latest，IfNotPresent被应用。 imagePullPolicy: Never：镜像被假设存在于本地。 没有尝试拉取镜像。</p>
<h1 id="7-3-第三步：使用Controllers管理Pod"><a href="#7-3-第三步：使用Controllers管理Pod" class="headerlink" title="7.3 第三步：使用Controllers管理Pod"></a>7.3 第三步：使用Controllers管理Pod</h1><p>在实际的生产环境中，我们其实很少单独创建Pod，而是通过控制器来进行Pod的管理，Kubernetes提供了很多的控制器，一个 Controllers 可以创建和管理很多个 Pod, 也提供复制、初始化，以及提供集群范围的自我恢复的功能。比如说： 如果一个节点宕机，Controller 将调度一个在其他节点上完全相同的 pod 来自动取代当前的 pod。</p>
<h1 id="8-1-Replication-Controller控制器"><a href="#8-1-Replication-Controller控制器" class="headerlink" title="8.1 Replication Controller控制器"></a>8.1 Replication Controller控制器</h1><h1 id="8-2-Replica-Sets控制器"><a href="#8-2-Replica-Sets控制器" class="headerlink" title="8.2 Replica Sets控制器"></a>8.2 Replica Sets控制器</h1><h1 id="8-3-Deployment控制器"><a href="#8-3-Deployment控制器" class="headerlink" title="8.3 Deployment控制器"></a>8.3 Deployment控制器</h1><h1 id="8-4-DaemonSet控制器"><a href="#8-4-DaemonSet控制器" class="headerlink" title="8.4 DaemonSet控制器"></a>8.4 DaemonSet控制器</h1><p>DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 使用 DaemonSet 的一些典型用法：</p>
<ul>
<li>运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。</li>
<li>在每个 Node 上运行日志收集 daemon，例如filebeat、logstash。</li>
<li>在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、Zabbix Agent。</li>
</ul>
<p><strong>创建DaemonSet</strong></p>
<p>DaemonSet的描述文件和Deployment非常相似，只需要修改Kind，并去掉副本数量的配置即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 example]# vim nginx-daemonset.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-daemonset</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.13.12</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>

<p>查看Pod在Node上的分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod -o wide</span><br><span class="line">NAME                    READY     STATUS    RESTARTS   AGE       IP           NODE</span><br><span class="line">nginx-daemonset-hk28q   1&#x2F;1       Running   0          1m        10.2.56.10   192.168.56.12</span><br><span class="line">nginx-daemonset-wtt68   1&#x2F;1       Running   0          1m        10.2.53.10   192.168.56.13</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl get daemonset</span><br><span class="line">NAME              DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">nginx-daemonset   2         2         2         2            2           &lt;none&gt;          1m</span><br></pre></td></tr></table></figure>

<h1 id="7-4-第四步：使用Service管理Pod访问"><a href="#7-4-第四步：使用Service管理Pod访问" class="headerlink" title="7.4 第四步：使用Service管理Pod访问"></a>7.4 第四步：使用Service管理Pod访问</h1><p>在上面的小节，我们通过Deployment可以为一个应用创建多个Pod，而且可以动态的进行增加、或者删除多余的Pod，Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，但是每次Pod的IP地址就会发生变化，外面如何访问到该Pod呢？总不能每次Pod重启就修改访问的IP地址吧。 每个 Pod 都会获取它自己的 IP 地址，但是每次即使这些 IP 地址不总是稳定可依赖的。 这会导致一个问题：在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么那些 frontend 该如何发现，并连接到这组 Pod 中的哪些 backend 呢？</p>
<h1 id="9-1-Service介绍和管理"><a href="#9-1-Service介绍和管理" class="headerlink" title="9.1 Service介绍和管理"></a>9.1 Service介绍和管理</h1><h3 id="创建Service"><a href="#创建Service" class="headerlink" title="创建Service"></a>创建Service</h3><p>继续我们Nginx的案例，我们为之前的应用创建一个Service</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cat nginx-service.yaml </span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-service</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure>

<ul>
<li>第1行：定义资源类型为Service</li>
<li>第2行：定义当前Service API的版本</li>
<li>第3行：metadata设置</li>
<li>第4行：设置Service的名称为nginx-service</li>
<li>第5行：spec: 开始设置Service的内容</li>
<li>第6行：selector: 为该Service指定一个匹配的标签</li>
<li>第7行：app: nginx 所有带有标签app ：nginx的Pod将使用该Service</li>
<li>第8行：ports: 指定Service需要对外的端口</li>
<li>第9行：设置端口协议：支持TCP和UDP</li>
<li>第10行：设置Service的端口</li>
<li>第11行：设置Pod的端口，Kubernetes会将发送给Service端口的连接，转发到Pod的端口上。</li>
</ul>
<p>创建Nginx Service</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl create -f nginx-service.yaml </span><br><span class="line">service &quot;nginx-service&quot; created</span><br></pre></td></tr></table></figure>

<p>查看Nginx Service</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get service</span><br><span class="line">NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes      ClusterIP   10.1.0.1      &lt;none&gt;        443&#x2F;TCP   7h</span><br><span class="line">nginx-service   ClusterIP   10.1.184.53   &lt;none&gt;        80&#x2F;TCP    25s</span><br></pre></td></tr></table></figure>

<p>访问Servce IP</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# curl --head 10.1.181.45</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.11.10</span><br><span class="line">Date: Tue, 21 Feb 2017 08:20:42 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 612</span><br><span class="line">Last-Modified: Tue, 14 Feb 2017 15:36:04 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">ETag: &quot;58a323e4-264&quot;</span><br><span class="line">Accept-Ranges: bytes</span><br></pre></td></tr></table></figure>

<h1 id="9-2-Service和Endpoint"><a href="#9-2-Service和Endpoint" class="headerlink" title="9.2 Service和Endpoint"></a>9.2 Service和Endpoint</h1><p>Service作为Kubernetes中为Pod实现负载均衡的组件，几乎在所有的文章中为了方便初学者理解，基本上说的是Service会来监听Pod的变化，然后来更新Pod的IP地址。其实这个事情不是Service干的，而是有一个幕后英雄：Endpoint Endpoints表示了一个Service对应的所有Pod副本的访问地址，而Endpoints Controller负责生成和维护所有Endpoints对象的控制器。它负责监听Service和对应的Pod副本的变化。</p>
<ul>
<li>如果监测到Service被删除，则删除和该Service同名的Endpoints对象；</li>
<li>如果监测到新的Service被创建或修改，则根据该Service信息获得相关的Pod列表，然后创建或更新Service对应的Endpoints对象。</li>
<li>如果监测到Pod的事件，则更新它对应的Service的Endpoints对象。</li>
</ul>
<p>kube-proxy进程获取每个Service的Endpoints，实现Service的负载均衡功能。</p>
<h3 id="创建一个Headless-Service"><a href="#创建一个Headless-Service" class="headerlink" title="创建一个Headless Service"></a>创建一个Headless Service</h3><p>编写一个Service不使用clusterip</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cat mysql-service.yaml </span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-service</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 3306</span><br><span class="line">    targetPort: 3306</span><br><span class="line">  clusterIP: None</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl create -f mysql-service.yaml </span><br><span class="line">service &quot;mysql-service&quot; created</span><br></pre></td></tr></table></figure>

<p>查看Service，可以放心CLUSTER-IP为None</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get service mysql-service</span><br><span class="line">NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">mysql-service   ClusterIP   None         &lt;none&gt;        3306&#x2F;TCP   17s</span><br></pre></td></tr></table></figure>

<p>2.创建一个Endpoint</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim mysql-endpoint.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line"> name: mysql-service</span><br><span class="line">subsets:</span><br><span class="line">- addresses:</span><br><span class="line">  - ip: 192.168.56.13</span><br><span class="line">  ports:</span><br><span class="line">  - port: 3306</span><br><span class="line">    protocol: TCP</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl create -f mysql-endpoint.yaml </span><br><span class="line">endpoints &quot;mysql-service&quot; created</span><br></pre></td></tr></table></figure>

<p>3.查看Service和Endpoint的关联</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get ep mysql-service</span><br><span class="line">NAME            ENDPOINTS            AGE</span><br><span class="line">mysql-service   192.168.56.13:3306   42s</span><br><span class="line">[root@linux-node1 ~]# kubectl describe svc mysql-service</span><br><span class="line">Name:              mysql-service</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          &lt;none&gt;</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                None</span><br><span class="line">Port:              &lt;unset&gt;  3306&#x2F;TCP</span><br><span class="line">TargetPort:        3306&#x2F;TCP</span><br><span class="line">Endpoints:         192.168.56.13:3306</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h1 id="7-5-第五步：使用Ingress提供外部访问"><a href="#7-5-第五步：使用Ingress提供外部访问" class="headerlink" title="7.5 第五步：使用Ingress提供外部访问"></a>7.5 第五步：使用Ingress提供外部访问</h1><p>通过Service可以将Kubernetes集群中的服务以IP：Port的方式暴露出来，我们称之为4层的负载均衡，因为这个是OSI七层模型中传输层的功能。</p>
<p>那么如何实现七层的负载均衡呢，例如像Nginx那样，可以灵活的进行反向代理的设置，根据不同的URL进行转发等，难道我需要自己部署一个Nginx来做这个事情吗？首先，如果你有这个想法，并没有错，甚至你完全可以自己独立部署一个Nginx来完成，但是Kubernetes提供了更好的解决方案就是Ingress。 Ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。ingress相当于nginx反向代理服务器，它包括的规则定义就是URL的路由信息。</p>
<h2 id="10-1-Ingress-Controller"><a href="#10-1-Ingress-Controller" class="headerlink" title="10.1 Ingress Controller"></a>10.1 Ingress Controller</h2><p>在学习Service的时候，我们知道最终的负载均衡由kube-proxy和LVS来完成，那么Ingress靠什么来实现7层的路由机制呢？答案是Ingress Controller。</p>
<p>Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟 kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少 pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。</p>
<p>Ingress Controller目前有两大开源项目，一个是Nginx Controller，一个是目前比较流行的Traefik，Traefik是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, Kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型。</p>
<h2 id="Ingress-Controller-Traefik"><a href="#Ingress-Controller-Traefik" class="headerlink" title="Ingress Controller Traefik"></a>Ingress Controller Traefik</h2><h3 id="部署Treafik"><a href="#部署Treafik" class="headerlink" title="部署Treafik"></a>部署Treafik</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl label nodes 192.168.56.12 edgenode&#x3D;true</span><br><span class="line">node &quot;192.168.56.12&quot; labeled</span><br><span class="line">[root@linux-node1 ~]# kubectl create -f &#x2F;srv&#x2F;addons&#x2F;ingress&#x2F;</span><br></pre></td></tr></table></figure>

<h1 id="11-第六步：使用PV和PVC管理数据存储"><a href="#11-第六步：使用PV和PVC管理数据存储" class="headerlink" title="11 第六步：使用PV和PVC管理数据存储"></a>11 第六步：使用PV和PVC管理数据存储</h1><p>截止目前我们所启动Pod的容器中的数据存储都是临时的，因此Pod重启或者被删除的时候，产生在容器中的数据会发生丢失。实际应用中，我们有些应用是无状态，有些应用则需要保持状态数据，确保Pod重启之后能够读取到之前的状态数据，有些应用则作为集群提供服务。这三种服务归纳为无状态服务、有状态服务以及有状态的集群服务，其中后面两个存在数据保存与共享的需求，因此就要采用容器外的存储方案。 Kubernetes中存储中有四个重要的概念：Volume、PersistentVolume（PV）、PersistentVolumeClaim （PVC）、StorageClass。掌握了这四个概念，就掌握了Kubernetes中存储系统的核心。</p>
<h1 id="11-1-Kubernetes-Volume"><a href="#11-1-Kubernetes-Volume" class="headerlink" title="11.1 Kubernetes Volume"></a>11.1 Kubernetes Volume</h1><h1 id="11-2-PersistentVolume（PV）"><a href="#11-2-PersistentVolume（PV）" class="headerlink" title="11.2 PersistentVolume（PV）"></a>11.2 PersistentVolume（PV）</h1><p>PersistentVolume（PV）是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、iSCSI 或特定于云供应商的存储系统。</p>
<p><strong>1.安装并配置NFS</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# yum install -y nfs-utils rpcbind</span><br><span class="line">[root@linux-node1 ~]# mkdir -p &#x2F;data&#x2F;k8s-nfs</span><br><span class="line">[root@linux-node1 ~]# vim &#x2F;etc&#x2F;exports</span><br><span class="line">&#x2F;data&#x2F;k8s-nfs *(rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure>

<p>启动NFS</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# systemctl enable rpcbind nfs</span><br><span class="line">[root@linux-node1 ~]# systemctl start rpcbind nfs</span><br></pre></td></tr></table></figure>

<p><strong>2.创建并查看PV</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim nfs-pv.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-demo</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">storage: 1Gi</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  persistentVolumeReclaimPolicy: Recycle</span><br><span class="line">  storageClassName: nfs</span><br><span class="line">  nfs:</span><br><span class="line">    path: &#x2F;data&#x2F;k8s-nfs&#x2F;pv-demo</span><br><span class="line">    server: 192.168.56.11</span><br><span class="line">[root@linux-node1 ~]# kubectl create -f nfs-pv.yaml </span><br><span class="line">persistentvolume &quot;pv-demo&quot; created</span><br></pre></td></tr></table></figure>

<p>查看创建的PV</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE</span><br><span class="line">pv-demo   1Gi        RWO            Recycle          Available             nfs                      15s</span><br></pre></td></tr></table></figure>

<h1 id="11-3-PersistentVolumeClaim（PVC）"><a href="#11-3-PersistentVolumeClaim（PVC）" class="headerlink" title="11.3 PersistentVolumeClaim（PVC）"></a>11.3 PersistentVolumeClaim（PVC）</h1><p>PersistentVolumeClaim（PVC）是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂载）。</p>
<p>1.创建PVC</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim nfs-pvc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-demo</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: nfs</span><br></pre></td></tr></table></figure>

<p>创建并查看PVC</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl create -f nfs-pvc.yaml </span><br><span class="line">persistentvolumeclaim &quot;pvc-demo&quot; created</span><br><span class="line">[root@linux-node1 ~]# kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-demo   Bound     pv-demo   1Gi        RWO            nfs            6s</span><br></pre></td></tr></table></figure>

<p>2.使用PVC</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim nginx-deployment-pvc.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.13.12</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: &quot;&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&quot;</span><br><span class="line">          name: pvc-demo</span><br><span class="line">      volumes:</span><br><span class="line">      - name: pvc-demo</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: pvc-demo</span><br></pre></td></tr></table></figure>

<h1 id="11-4-StorageClass"><a href="#11-4-StorageClass" class="headerlink" title="11.4 StorageClass"></a>11.4 StorageClass</h1><h1 id="7-第七步：使用Rancher管理Kubernetes集群"><a href="#7-第七步：使用Rancher管理Kubernetes集群" class="headerlink" title="7 第七步：使用Rancher管理Kubernetes集群"></a>7 第七步：使用Rancher管理Kubernetes集群</h1><p>现在我们已经具备把应用迁移到Kubernetes中来的能力，那么现在，迁移后，日常的运维工作就发生的变化，截止目前，在生产环境中，我们很少使用官方自带的Dashbaord来完成日常的运维工作，而是使用第三方的运维工具Rancher。</p>
<h2 id="7-1-Rancher快速入门"><a href="#7-1-Rancher快速入门" class="headerlink" title="7.1 Rancher快速入门"></a>7.1 Rancher快速入门</h2><p>快速部署单机版Rancher</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# mkdir &#x2F;opt&#x2F;rancher</span><br><span class="line">[root@linux-node1 ~]# docker run -d --restart&#x3D;unless-stopped -v &#x2F;opt&#x2F;rancher:&#x2F;var&#x2F;lib&#x2F;rancher&#x2F; -p 80:80 -p 443:443 rancher&#x2F;rancher</span><br></pre></td></tr></table></figure>

<h2 id="7-2-使用Rancher进行日常管理"><a href="#7-2-使用Rancher进行日常管理" class="headerlink" title="7.2 使用Rancher进行日常管理"></a>7.2 使用Rancher进行日常管理</h2><h2 id="7-3-Rancher生产集群部署"><a href="#7-3-Rancher生产集群部署" class="headerlink" title="7.3 Rancher生产集群部署"></a>7.3 Rancher生产集群部署</h2><h1 id="第六部分-管理Kubernetes中的应用"><a href="#第六部分-管理Kubernetes中的应用" class="headerlink" title="第六部分 管理Kubernetes中的应用"></a>第六部分 管理Kubernetes中的应用</h1><h1 id="13-应用的资源限制和健康检查"><a href="#13-应用的资源限制和健康检查" class="headerlink" title="13 应用的资源限制和健康检查"></a>13 应用的资源限制和健康检查</h1><h1 id="13-1-应用的资源限制"><a href="#13-1-应用的资源限制" class="headerlink" title="13.1 应用的资源限制"></a>13.1 应用的资源限制</h1><h1 id="13-2-应用的健康检查"><a href="#13-2-应用的健康检查" class="headerlink" title="13.2 应用的健康检查"></a>13.2 应用的健康检查</h1><h3 id="Liveness探测"><a href="#Liveness探测" class="headerlink" title="Liveness探测"></a>Liveness探测</h3><p>Kubernetes有两种探测机制，Liveness和Readiness，配置都是相似的，只是实现的功能不同。 Liveness探测是针对Pod健康状态的探测，类似于集群中的健康检查，用户可以自定义这个健康检查的条件，如果探测失败，Kubernetes将会重启容器。 如果您希望容器在探测失败时被杀死并重新启动，那么请指定一个Liveness配置，并指定restartPolicy 为 Always 或 OnFailure。 配置案例</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">livenessProbe:</span><br><span class="line">exec:</span><br><span class="line">    command:</span><br><span class="line">    - ps aux | grep nginx</span><br><span class="line">initialDelaySeconds: 10</span><br><span class="line">periodSeconds: 5</span><br><span class="line">timeoutSeconds: 3</span><br></pre></td></tr></table></figure>

<h3 id="Readiness探测"><a href="#Readiness探测" class="headerlink" title="Readiness探测"></a>Readiness探测</h3><p>Readiness探测是探测Pod是否准备好对外提供访问，例如我们在Pod里面运行一个Tomcat的容器，里面运行了一个Jenkins的应用，那么等Jenkins完全启动能提供服务可能需要1分钟，所以在在1分钟之前是不能提供给用户访问的，也就是不能加入Service的负载均衡中，这个就靠Readiness探测来控制。 Readiness用来控制告诉Kubernetes什么时间可以将容器加入到Service的负载均衡中，配置方法和Liveness一样，只需要修改livennessProbe替换为readinessProbe即可。</p>
<h2 id="健康检查的方法"><a href="#健康检查的方法" class="headerlink" title="健康检查的方法"></a>健康检查的方法</h2><p>Kubernetes的健康检查是由运行在各个Node上的kubelet来完成的，kubelet目前支持以下三种健康检查的方法：</p>
<ul>
<li>ExecAction：在容器中执行指定的命令。如果命令退出时状态码为0，则认为诊断成功。</li>
<li>TCPSocketAction:对指定端口上容器的IP地址执行TCP检查。如果端口是打开的，则认为诊断是成功的。</li>
<li>HTTPGetAction:对指定端口和路径上容器的IP地址执行HTTP Get请求。如果响应的状态码大于或等于200，小于400，则认为诊断是成功的。</li>
</ul>
<p>以上三种健康检查的方法会有三种返回结果：</p>
<ul>
<li><p>Success：成功，容器通过诊断。</p>
</li>
<li><p>Failure：失败，容器诊断失败。</p>
</li>
<li><p>Unknown：探测失败，无法执行探测，所以不应该采取任何行动。</p>
<p>针对于探针有以下配置参数，需要注意不管是Liveness还是Readiness探测，探针的使用都是相同的，唯一的不同是探测完毕后，执行操作的不同。</p>
</li>
<li><p>initialDelaySeconds: 探测的延迟时间，单位是秒。也就是说在容器启动多少秒之后开始进行第一次探测，例如你启动一个Java的应用需要50秒，那么这个值就需要大于50s。所以这个值是需要根据应用的具体情况来设置。</p>
</li>
<li><p>periodSeconds：探测执行的周期时间，单位是秒。是指每隔多长时间执行一次探测，频率越高，发现故障的时间也就越短，并不是越短越好。如果应用服务不够稳定，太高的频率反而会导致很多你认为的“误报”。默认是10秒，最小值是1秒。</p>
</li>
<li><p>timeoutSeconds: 探测超时时间，单位是秒，执行探测如果超过这个时间没有返回结果，变意味着探测的结果是失败。默认为1秒。最小值是1秒。</p>
</li>
<li><p>failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。这个是Kubernetes将在放弃之前尝试失败阈值时间。放弃生命探测意味着重新启动Pod。一旦准备就绪，Pod将被标记为未准备就绪。默认为3。最小值是1。</p>
</li>
<li><p>successThreshold: 在探测失败后，最少连续探测成功多少次才被认定为成功。默认为1，也就是必须探测成功1次，才能认为状态恢复，最小值是1。</p>
<h1 id="管理应用的DNS访问"><a href="#管理应用的DNS访问" class="headerlink" title="管理应用的DNS访问"></a>管理应用的DNS访问</h1></li>
</ul>
<h1 id="14-1-Kubernetes中的DNS"><a href="#14-1-Kubernetes中的DNS" class="headerlink" title="14.1 Kubernetes中的DNS"></a><a target="_blank" rel="noopener external nofollow noreferrer" href="http://k8s.unixhot.com/">14.1 Kubernetes中的DNS</a></h1><h1 id="应用的DNS管理"><a href="#应用的DNS管理" class="headerlink" title="应用的DNS管理"></a>应用的DNS管理</h1><h3 id="Pod的域名解析策略"><a href="#Pod的域名解析策略" class="headerlink" title="Pod的域名解析策略"></a>Pod的域名解析策略</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl run dns-test --generator&#x3D;run-pod&#x2F;v1 --image&#x3D;alpine --replicas&#x3D;1 sleep 360000</span><br><span class="line">pod&#x2F;dns-test created</span><br></pre></td></tr></table></figure>

<p>查看Pod</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod dns-test</span><br><span class="line">NAME       READY   STATUS    RESTARTS   AGE</span><br><span class="line">dns-test   1&#x2F;1     Running   0          79s</span><br></pre></td></tr></table></figure>

<p>Pod默认的DNS配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl exec -it dns-test &#x2F;bin&#x2F;sh</span><br><span class="line">&#x2F; # cat &#x2F;etc&#x2F;resolv.conf </span><br><span class="line">nameserver 10.1.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>

<p>如何访问Service名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#x2F; # ping -c 3 wordpress-service.default.svc.cluster.local</span><br><span class="line">PING wordpress-service.default.svc.cluster.local (10.1.92.244): 56 data bytes</span><br><span class="line">64 bytes from 10.1.92.244: seq&#x3D;0 ttl&#x3D;64 time&#x3D;0.074 ms</span><br><span class="line">64 bytes from 10.1.92.244: seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.141 ms</span><br><span class="line">64 bytes from 10.1.92.244: seq&#x3D;2 ttl&#x3D;64 time&#x3D;0.187 ms</span><br><span class="line"></span><br><span class="line">--- wordpress-service.default.svc.cluster.local ping statistics ---</span><br><span class="line">3 packets transmitted, 3 packets received, 0% packet loss</span><br><span class="line">round-trip min&#x2F;avg&#x2F;max &#x3D; 0.074&#x2F;0.134&#x2F;0.187 ms</span><br></pre></td></tr></table></figure>

<p>DNS查询策略</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod dns-test -o yaml | grep dnsPolicy</span><br><span class="line">  dnsPolicy: ClusterFirst</span><br></pre></td></tr></table></figure>

<ul>
<li>Default: Pod从其运行的节点中继承名称解析配置。</li>
<li>ClusterFirst:（默认策略）与配置的群集域名后缀不匹配的任何DNS查询都将转发到从节点继承的上游名称服务器。</li>
<li>ClusterFirstWithHostNet: 如果Pod使用了hostNetwork（例如Ingress Controller Treafik就是使用了hostNetwok），应显式设置其DNS策略为“ClusterFirstWithHostNet”。</li>
<li>None: 它允许Pod忽略Kubernetes环境中的DNS设置，这时候会使用Pod Spec中的dnsConfig字段提供的DNS设置。</li>
</ul>
<h2 id="应用的DNS管理-1"><a href="#应用的DNS管理-1" class="headerlink" title="应用的DNS管理"></a>应用的DNS管理</h2><h2 id="15-1-使用ConfigMap管理应用配置"><a href="#15-1-使用ConfigMap管理应用配置" class="headerlink" title="15.1 使用ConfigMap管理应用配置"></a>15.1 使用ConfigMap管理应用配置</h2><h3 id="通过kubectl命令创建ConfigMap"><a href="#通过kubectl命令创建ConfigMap" class="headerlink" title="通过kubectl命令创建ConfigMap"></a>通过kubectl命令创建ConfigMap</h3><p><strong>创建一个名称为cmd-config的ConfigMap</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl create configmap cmd-config --from-literal&#x3D;host&#x3D;www.unixhot.com</span><br><span class="line">configmap&#x2F;cmd-config created</span><br></pre></td></tr></table></figure>

<p><strong>查看ConfigMap</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get configmap</span><br><span class="line">NAME         DATA   AGE</span><br><span class="line">cmd-config   1      63s</span><br></pre></td></tr></table></figure>

<p><strong>查看ConfigMap内容</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl describe configmap cmd-config</span><br><span class="line">Name:         cmd-config</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">host:</span><br><span class="line">----</span><br><span class="line">www.unixhot.com</span><br><span class="line">Events:  &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p><strong>ConfigMap中包含多个键值对</strong> 可以多次使用–from-literal为一个ConfigMap创建多个键值对，中间用空格分隔</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl create configmap mcmd-config --from-literal&#x3D;host&#x3D;www.unixhot.com --from-literal&#x3D;port&#x3D;443 --from-literal&#x3D;ssl&#x3D;on</span><br><span class="line">configmap&#x2F;mcmd-config created</span><br></pre></td></tr></table></figure>

<h3 id="通过YAML文件创建ConfigMap"><a href="#通过YAML文件创建ConfigMap" class="headerlink" title="通过YAML文件创建ConfigMap"></a>通过YAML文件创建ConfigMap</h3><p>查看已创建的ConfigMap生成的YAML文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get configmap mcmd-config -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  host: www.unixhot.com</span><br><span class="line">  port: &quot;443&quot;</span><br><span class="line">  ssl: &quot;on&quot;</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2019-11-05T01:45:13Z&quot;</span><br><span class="line">  name: mcmd-config</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;5394993&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;configmaps&#x2F;mcmd-config</span><br><span class="line">  uid: 02012d69-e324-4e9d-ba04-7132e9f6ecd8</span><br></pre></td></tr></table></figure>

<p>只需要将metadata中无需指定的字段去掉即可生成一个YAML文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get configmap mcmd-config -o yaml &gt; mcmd-config-v2.yaml</span><br><span class="line">[root@k8s-master1 ~]# vim mcmd-config-v2.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  host: www.unixhot.com</span><br><span class="line">  port: &quot;443&quot;</span><br><span class="line">  ssl: &quot;on&quot;</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: mcmd-config-v2</span><br><span class="line">  namespace: default</span><br></pre></td></tr></table></figure>

<p>注意需要修改metadata.name,修改完毕后直接创建即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl create -f mcmd-config-v2.yaml </span><br><span class="line">configmap&#x2F;mcmd-config-v2 created</span><br><span class="line">[root@k8s-master1 ~]# kubectl get configmap</span><br><span class="line">NAME             DATA   AGE</span><br><span class="line">cmd-config       1      24m</span><br><span class="line">mcmd-config      3      16m</span><br><span class="line">mcmd-config-v2   3      9s</span><br></pre></td></tr></table></figure>

<h3 id="通过文件创建ConfigMap"><a href="#通过文件创建ConfigMap" class="headerlink" title="通过文件创建ConfigMap"></a>通过文件创建ConfigMap</h3><p>ConfigMap除了可以存储单个或者多个键值对之外，可以存储完整的配置文件，将单个配置文件直接转换为ConfigMap在生产中十分常用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl create configmap file-config --from-file&#x3D;&#x2F;etc&#x2F;hosts</span><br><span class="line">configmap&#x2F;file-config created</span><br></pre></td></tr></table></figure>

<p>可以看到ConfigMap直接存储了文件的内容，Key名称为文件名hosts，也可以手动指定Key的名称。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl describe configmap file-config</span><br><span class="line">Name:         file-config</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">hosts:</span><br><span class="line">----</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.99.27 k8s-master1 k8s-master1.dianjoy.com </span><br><span class="line">192.168.99.28 k8s-master2 k8s-master2.dianjoy.com</span><br><span class="line">192.168.99.29 k8s-master3 k8s-master3.dianjoy.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Events:  &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>将Key手动指定为host-hosts</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl create configmap file-config-v2 --from-file&#x3D;host-hosts&#x3D;&#x2F;etc&#x2F;hosts</span><br><span class="line">configmap&#x2F;file-config-v2 created</span><br><span class="line">[root@k8s-master1 ~]# kubectl describe configmap file-config-v2</span><br><span class="line">Name:         file-config-v2</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">host-hosts:</span><br><span class="line">----</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.99.27 k8s-master1 k8s-master1.dianjoy.com </span><br><span class="line">192.168.99.28 k8s-master2 k8s-master2.dianjoy.com</span><br><span class="line">192.168.99.29 k8s-master3 k8s-master3.dianjoy.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Events:  &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h3 id="从目录创建ConfigMap"><a href="#从目录创建ConfigMap" class="headerlink" title="从目录创建ConfigMap"></a>从目录创建ConfigMap</h3><p>ConfigMap还支持通过目录创建，kubectl会为目录中的每个文件单独创建条目，需要注意的是如果目录下面包含子目录，会忽略这些子目录和子目录里面的内容。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl create configmap dir-config --from-file&#x3D;&#x2F;etc&#x2F;kubernetes</span><br><span class="line">configmap&#x2F;dir-config created</span><br></pre></td></tr></table></figure>

<h3 id="混合选项创建ConfigMap"><a href="#混合选项创建ConfigMap" class="headerlink" title="混合选项创建ConfigMap"></a>混合选项创建ConfigMap</h3><p>同时使用命令行、文件、目录创建ConfigMap也是支持的，只需要使用不同的选项即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl create configmap mycp --from-literal&#x3D;env&#x3D;test \</span><br><span class="line"> --from-file&#x3D;&#x2F;etc&#x2F;hosts \</span><br><span class="line"> --from-file&#x3D;myhosts&#x3D;&#x2F;etc&#x2F;hosts \</span><br><span class="line"> --from-file&#x3D;&#x2F;etc&#x2F;kubernetes</span><br><span class="line">configmap&#x2F;mycp created</span><br></pre></td></tr></table></figure>

<p>ConfigMap的内容可以通过环境变量的形成传递给容器，也可通过和Volume的形式挂载到容器中。</p>
<h3 id="通过环境变量给容器传递ConfigMap"><a href="#通过环境变量给容器传递ConfigMap" class="headerlink" title="通过环境变量给容器传递ConfigMap"></a>通过环境变量给容器传递ConfigMap</h3><p>可以将ConfigMap中的键值对数据通过环境变量的形式传递到容器中，这样在配置容器的时候有一些数据可以使用环境变量，然后使用ConfigMap进行填充，这样就可以实现配置和Pod的分离。</p>
<h2 id="15-2-使用Secret管理敏感数据"><a href="#15-2-使用Secret管理敏感数据" class="headerlink" title="15.2 使用Secret管理敏感数据"></a>15.2 使用Secret管理敏感数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在应用启动过程中经常会有一些敏感信息需要存储，例如用户名和密码等，如果直接明文的方式保存会有安全风险。在Kubernetes中Secret这个资源对象类型用来保存敏感信息，例如密码、密钥、访问令牌、SSH Key等你认为需要保密的敏感信息。相对于将这些内容保存到容器镜像或者Pod的定义文件中，更加的灵活和安全。</span><br></pre></td></tr></table></figure>

<h3 id="配置Pod使用Harbor镜像"><a href="#配置Pod使用Harbor镜像" class="headerlink" title="配置Pod使用Harbor镜像"></a>配置Pod使用Harbor镜像</h3><p>1．docker login得到 docker密码文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# docker login 192.168.56.11</span><br></pre></td></tr></table></figure>

<p>2.对密码文件进行加密</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cat &#x2F;root&#x2F;.docker&#x2F;config.json |base64</span><br></pre></td></tr></table></figure>

<p>3.创建harbor使用的Secret YAML文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim harbor-secret.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: harbor-secret</span><br><span class="line">  namespace: default</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: &#39;ewoJImF1dGhzIjogewoJCSJyZWcuZ3JlYXRvcHMubmV0IjogewoJCQkiYXV0aCI6ICJZV1J0YVc0Nk1YRmhlbmh6ZHpJPSIKCQl9Cgl9Cn0&#x3D;&#39;</span><br><span class="line">type: kubernetes.io&#x2F;dockerconfigjson</span><br></pre></td></tr></table></figure>

<p>4.创建Secret</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@jenkins k8s-deploy]# kubectl create -f reg-harbor.yaml </span><br><span class="line">secret &quot;reg-harbor&quot; created</span><br></pre></td></tr></table></figure>

<p>5.创建pod并挂载资源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: sectest</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: sectest</span><br><span class="line">    image: 123.207.154.16&#x2F;base&#x2F;redis:alpine</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 6379</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">    - name: harbor-secret</span><br></pre></td></tr></table></figure>

<h1 id="使用Helm管理Kubernetes应用"><a href="#使用Helm管理Kubernetes应用" class="headerlink" title="使用Helm管理Kubernetes应用"></a>使用Helm管理Kubernetes应用</h1><p>通过前面的学习，掌握了将应用迁移至Kubernetes的步骤和技巧，过程比较艰辛。例如我们创建一个应用，涉及到Deployment、Service、Ingress、PV、PVC，如何有效的管理这些资源呢，Kubernetes给出了一个最佳实践就是Helm。 Helm是一个kubernetes应用的包管理工具，用来管理charts——预先配置好的安装包资源，有点类似于Ubuntu的APT和CentOS中的yum。 Helm chart是用来封装kubernetes原生应用程序的yaml文件，可以在你部署应用的时候自定义应用程序的一些metadata，便与应用程序的分发。</p>
<p>Helm和charts的主要作用：</p>
<ul>
<li>应用程序封装</li>
<li>版本管理</li>
<li>依赖检查</li>
<li>便于应用程序分发</li>
</ul>
<h2 id="Helm部署"><a href="#Helm部署" class="headerlink" title="Helm部署"></a>Helm部署</h2><h3 id="安装Helm"><a href="#安装Helm" class="headerlink" title="安装Helm"></a>安装Helm</h3><p>1.部署Helm客户端</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd &#x2F;usr&#x2F;local&#x2F;src</span><br><span class="line">[root@linux-node1 src]# wget https:&#x2F;&#x2F;get.helm.sh&#x2F;helm-v3.0.2-linux-amd64.tar.gz</span><br><span class="line">[root@linux-node1 src]# tar zxf helm-v3.0.2-linux-amd64.tar.gz</span><br><span class="line">[root@linux-node1 src]# mv linux-amd64&#x2F;helm &#x2F;usr&#x2F;local&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure>

<p>2.验证安装是否成功</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm version</span><br><span class="line">version.BuildInfo&#123;Version:&quot;v3.0.2&quot;, GitCommit:&quot;19e47ee3283ae98139d98460de796c1be1e3975f&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.13.5&quot;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="使用Helm部署第一个应用"><a href="#使用Helm部署第一个应用" class="headerlink" title="使用Helm部署第一个应用"></a>使用Helm部署第一个应用</h3><p>4.搜索Helm应用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm search jenkins</span><br><span class="line">NAME              CHART VERSION    APP VERSION    DESCRIPTION                                       </span><br><span class="line">stable&#x2F;jenkins    0.13.5           2.73           Open source continuous integration server. It s...</span><br></pre></td></tr></table></figure>

<p>5.查看仓库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm repo list</span><br><span class="line">NAME      URL                                                   </span><br><span class="line">stable    https:&#x2F;&#x2F;kubernetes.oss-cn-hangzhou.aliyuncs.com&#x2F;charts</span><br><span class="line">local     http:&#x2F;&#x2F;127.0.0.1:8879&#x2F;charts</span><br></pre></td></tr></table></figure>

<p>6.安装第一个应用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm install stable&#x2F;jenkins</span><br><span class="line">NAME:   viable-seal</span><br><span class="line">LAST DEPLOYED: Thu Jul 26 19:21:07 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;ConfigMap</span><br><span class="line">NAME                       DATA  AGE</span><br><span class="line">viable-seal-jenkins        3     1s</span><br><span class="line">viable-seal-jenkins-tests  1     1s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;PersistentVolumeClaim</span><br><span class="line">NAME                 STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">viable-seal-jenkins  Pending  1s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME                       TYPE          CLUSTER-IP   EXTERNAL-IP  PORT(S)         AGE</span><br><span class="line">viable-seal-jenkins-agent  ClusterIP     10.1.154.54  &lt;none&gt;       50000&#x2F;TCP       1s</span><br><span class="line">viable-seal-jenkins        LoadBalancer  10.1.63.24   &lt;pending&gt;    8080:20031&#x2F;TCP  0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Deployment</span><br><span class="line">NAME                 DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">viable-seal-jenkins  1        1        1           0          0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                                  READY  STATUS   RESTARTS  AGE</span><br><span class="line">viable-seal-jenkins-7f5c7bd8d4-gc5hv  0&#x2F;1    Pending  0         0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Secret</span><br><span class="line">NAME                 TYPE    DATA  AGE</span><br><span class="line">viable-seal-jenkins  Opaque  2     1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get your &#39;admin&#39; user password by running:</span><br><span class="line">  printf $(kubectl get secret --namespace default viable-seal-jenkins -o jsonpath&#x3D;&quot;&#123;.data.jenkins-admin-password&#125;&quot; | base64 --decode);echo</span><br><span class="line">2. Get the Jenkins URL to visit by running these commands in the same shell:</span><br><span class="line">  NOTE: It may take a few minutes for the LoadBalancer IP to be available.</span><br><span class="line">        You can watch the status of by running &#39;kubectl get svc --namespace default -w viable-seal-jenkins&#39;</span><br><span class="line">  export SERVICE_IP&#x3D;$(kubectl get svc --namespace default viable-seal-jenkins --template &quot;&#123;&#123; range (index .status.loadBalancer.ingress 0) &#125;&#125;&#123;&#123; . &#125;&#125;&#123;&#123; end &#125;&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$SERVICE_IP:8080&#x2F;login</span><br><span class="line"></span><br><span class="line">3. Login with the password from step 1 and the username: admin</span><br><span class="line"></span><br><span class="line">For more information on running Jenkins on Kubernetes, visit:</span><br><span class="line">https:&#x2F;&#x2F;cloud.google.com&#x2F;solutions&#x2F;jenkins-on-container-engine</span><br></pre></td></tr></table></figure>

<h1 id="15-2-深入理解Helm"><a href="#15-2-深入理解Helm" class="headerlink" title="15.2 深入理解Helm"></a>15.2 深入理解Helm</h1><h3 id="Helm组件"><a href="#Helm组件" class="headerlink" title="Helm组件"></a>Helm组件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# tree ~&#x2F;.helm&#x2F;</span><br><span class="line">&#x2F;root&#x2F;.helm&#x2F;</span><br><span class="line">├── cache</span><br><span class="line">│   └── archive</span><br><span class="line">│       └── jenkins-0.13.5.tgz</span><br><span class="line">├── plugins</span><br><span class="line">├── repository</span><br><span class="line">│   ├── cache</span><br><span class="line">│   │   ├── local-index.yaml -&gt; &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;local&#x2F;index.yaml</span><br><span class="line">│   │   └── stable-index.yaml</span><br><span class="line">│   ├── local</span><br><span class="line">│   │   └── index.yaml</span><br><span class="line">│   └── repositories.yaml</span><br><span class="line">└── starters</span><br><span class="line"></span><br><span class="line">7 directories, 5 files</span><br></pre></td></tr></table></figure>

<p>默认缓存的文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd .helm&#x2F;cache&#x2F;archive&#x2F;</span><br><span class="line">[root@linux-node1 archive]# ls -l</span><br><span class="line">total 16</span><br><span class="line">-rw-r--r-- 1 root root 12650 Jul 26 19:21 jenkins-0.13.5.tgz</span><br><span class="line"></span><br><span class="line">[root@linux-node1 archive]# tar zxf jenkins-0.13.5.tgz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@linux-node1 archive]# mv jenkins ~&#x2F;.helm&#x2F;repository&#x2F;local&#x2F;</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# cd ~&#x2F;.helm&#x2F;repository&#x2F;local&#x2F;jenkins&#x2F;</span><br><span class="line">[root@linux-node1 jenkins]# tree</span><br><span class="line">.</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── OWNERS</span><br><span class="line">├── README.md</span><br><span class="line">├── templates</span><br><span class="line">│   ├── config.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── home-pvc.yaml</span><br><span class="line">│   ├── jenkins-agent-svc.yaml</span><br><span class="line">│   ├── jenkins-master-deployment.yaml</span><br><span class="line">│   ├── jenkins-master-ingress.yaml</span><br><span class="line">│   ├── jenkins-master-networkpolicy.yaml</span><br><span class="line">│   ├── jenkins-master-svc.yaml</span><br><span class="line">│   ├── jenkins-test.yaml</span><br><span class="line">│   ├── jobs.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── rbac.yaml</span><br><span class="line">│   ├── secret.yaml</span><br><span class="line">│   ├── service-account.yaml</span><br><span class="line">│   └── test-config.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">1 directory, 19 files</span><br></pre></td></tr></table></figure>

<h3 id="自定义Jenkins的Chart"><a href="#自定义Jenkins的Chart" class="headerlink" title="自定义Jenkins的Chart"></a>自定义Jenkins的Chart</h3><p>修改为NodePort</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 jenkins]# vim values.yaml</span><br><span class="line">ServiceType: NodePort</span><br></pre></td></tr></table></figure>

<p>检查</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm lint ~&#x2F;.helm&#x2F;repository&#x2F;local&#x2F;jenkins&#x2F;</span><br><span class="line">&#x3D;&#x3D;&gt; Linting &#x2F;root&#x2F;.helm&#x2F;repository&#x2F;local&#x2F;jenkins&#x2F;</span><br><span class="line">Lint OK</span><br><span class="line"></span><br><span class="line">1 chart(s) linted, no failures</span><br></pre></td></tr></table></figure>

<p>查看有哪些应用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm list</span><br><span class="line">NAME           REVISION    UPDATED                     STATUS      CHART          NAMESPACE</span><br><span class="line">viable-seal    1           Thu Jul 26 19:21:07 2018    DEPLOYED    jenkins-0.13.5 default</span><br><span class="line">[root@linux-node1 ~]# helm delete --purge viable-seal</span><br><span class="line">release &quot;viable-seal&quot; deleted</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# helm install ~&#x2F;.helm&#x2F;repository&#x2F;local&#x2F;jenkins&#x2F; --name devops-jenkins </span><br><span class="line">NAME:   devops-jenkins</span><br><span class="line">LAST DEPLOYED: Thu Jul 26 19:36:10 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Secret</span><br><span class="line">NAME            TYPE    DATA  AGE</span><br><span class="line">devops-jenkins  Opaque  2     0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;ConfigMap</span><br><span class="line">NAME                  DATA  AGE</span><br><span class="line">devops-jenkins        3     0s</span><br><span class="line">devops-jenkins-tests  1     0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;PersistentVolumeClaim</span><br><span class="line">NAME            STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">devops-jenkins  Pending  0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME                  TYPE       CLUSTER-IP   EXTERNAL-IP  PORT(S)         AGE</span><br><span class="line">devops-jenkins-agent  ClusterIP  10.1.74.175  &lt;none&gt;       50000&#x2F;TCP       0s</span><br><span class="line">devops-jenkins        NodePort   10.1.3.112   &lt;none&gt;       8080:23558&#x2F;TCP  0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Deployment</span><br><span class="line">NAME            DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">devops-jenkins  1        1        1           0          0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                            READY  STATUS   RESTARTS  AGE</span><br><span class="line">devops-jenkins-64d54b79c-pwjfb  0&#x2F;1    Pending  0         0s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get your &#39;admin&#39; user password by running:</span><br><span class="line">  printf $(kubectl get secret --namespace default devops-jenkins -o jsonpath&#x3D;&quot;&#123;.data.jenkins-admin-password&#125;&quot; | base64 --decode);echo</span><br><span class="line">2. Get the Jenkins URL to visit by running these commands in the same shell:</span><br><span class="line">  export NODE_PORT&#x3D;$(kubectl get --namespace default -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services devops-jenkins)</span><br><span class="line">  export NODE_IP&#x3D;$(kubectl get nodes --namespace default -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT&#x2F;login</span><br><span class="line"></span><br><span class="line">3. Login with the password from step 1 and the username: admin</span><br><span class="line"></span><br><span class="line">For more information on running Jenkins on Kubernetes, visit:</span><br><span class="line">https:&#x2F;&#x2F;cloud.google.com&#x2F;solutions&#x2F;jenkins-on-container-engine</span><br></pre></td></tr></table></figure>

<p>查看状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm status devops-jenkins</span><br></pre></td></tr></table></figure>

<h2 id="创建自己的Chart"><a href="#创建自己的Chart" class="headerlink" title="创建自己的Chart"></a>创建自己的Chart</h2><h3 id="创建自定义Nginx的Chart"><a href="#创建自定义Nginx的Chart" class="headerlink" title="创建自定义Nginx的Chart"></a>创建自定义Nginx的Chart</h3><p>1.创建自定义Chart Nginx的结构</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm create helm-nginx</span><br><span class="line">Creating helm-nginx</span><br><span class="line"> [root@linux-node1 ~]# tree helm-nginx&#x2F;</span><br><span class="line">opencmdb&#x2F;</span><br><span class="line">├── charts       #依赖的chart</span><br><span class="line">├── Chart.yaml   #本chart的信息</span><br><span class="line">├── templates    #模板目录</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   └── service.yaml</span><br><span class="line">└── values.yaml   #模板赋值</span><br></pre></td></tr></table></figure>

<p>2.编辑Chart配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd helm-nginx&#x2F;</span><br><span class="line">[root@linux-node1 helm-nginx]# vim values.yaml</span><br></pre></td></tr></table></figure>

<p>3.验证Chart配置，最后面的点表示当前目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 helm-nginx]# helm install --dry-run --debug --name helm-nginx .</span><br></pre></td></tr></table></figure>

<p>4.安装自定义Chart，最后面的点表示当前目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 helm-nginx]# helm install --name helm-nginx .</span><br></pre></td></tr></table></figure>

<h3 id="查看Helm实例"><a href="#查看Helm实例" class="headerlink" title="查看Helm实例"></a>查看Helm实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# helm list</span><br><span class="line">NAME          REVISION    UPDATED                     STATUS      CHART                 NAMESPACE</span><br><span class="line">helm-nginx    1           Sun Sep 16 19:32:19 2018    DEPLOYED    helm-nginx-0.1.0      default  </span><br><span class="line">[root@linux-node1 ~]# kubectl get pod</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">helm-nginx-6975f8dbcd-htvtd   1&#x2F;1       Running   0          51s</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl get ingress</span><br><span class="line">NAME         HOSTS                ADDRESS   PORTS     AGE</span><br><span class="line">helm-nginx   www.helm-nginx.com             80        1m</span><br></pre></td></tr></table></figure>

<h1 id="17-应用的日志采集与分析"><a href="#17-应用的日志采集与分析" class="headerlink" title="17 应用的日志采集与分析"></a>17 应用的日志采集与分析</h1><h2 id="Prometheus快速入门"><a href="#Prometheus快速入门" class="headerlink" title="Prometheus快速入门"></a>Prometheus快速入门</h2><h3 id="Prometheus架构介绍"><a href="#Prometheus架构介绍" class="headerlink" title="Prometheus架构介绍"></a>Prometheus架构介绍</h3><p>在学习Prometheus之前我们需要清晰的掌握其架构，Prometheus是由多个组件组成的的监控系统，主要有：Prometheus Server、Alertmanager、Pushgateway组成，这三个组件均为独立的应用服务，独立部署和运行，其中Prometheus Server中内置了Prometheus web UI。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/2ada1ece66fcc81d704c2ba46f9dd7d3.png" alt="architecture"></p>
<p><strong>Prometheus Server：</strong></p>
<p>Promethedus Server是核心组件，负责数据的获取、存储、查询。Prometheus通过Pull的方式定期的从Jobs/Exporters中获取数据，并保存在内置的TSDB中；内置的Prometheus web UI可以让用户通过PromQL的方式进行数据的检索。</p>
<p><strong>Exporters：</strong></p>
<p>Exporters也是一个独立的组件，有官方提供的Exporters也有社区贡献的Exportes，它将监控采集的数据通过HTTP的方式暴露给Prometheus Server，Server定期获取数据。例如有一个Exporters叫做Node Exporter，它安装在受采集的主机上，为Server提供数据，有点类似于Zabbix监控系统中的Zabbix Agent。</p>
<p><strong>Prometheus web UI：</strong></p>
<p>Prometheus web UI是Server启动后内置的一个Web界面，通过该Web界面我们可以进行数据查询工作，不包含设置的相关功能。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/cc11e0cf02ff729fb905ac3648af18f7.png" alt="img"></p>
<p><strong>PromQL：</strong></p>
<p>PromQL是Prometheus内置的自定义的查询语言，提供对Prometheus Server中的TSDB这个时间序列数据库进行数据查询，支持数据聚合和一些逻辑运算，是一个相对简单的查询语言，而且PromQL也提供了一些内置函数，帮助我们进行数据处理。</p>
<p><strong>Alertmanager：</strong></p>
<p>Alertmanager是Promethedus的告警管理组件，它支持基于PromQL来创建告警规则，类似于Zabbix中的告警表达式，对获取到的数据进行计算和比较，如果满足PromQL定义的规则条件，就会产生报警。</p>
<p><strong>Pushgateway：</strong></p>
<p>Pushgateway可以理解为数据的一个中转站，例如当Prometheus Server不能直接和Exporters进行通信的场景下。</p>
<h3 id="安装Prometheus"><a href="#安装Prometheus" class="headerlink" title="安装Prometheus"></a>安装Prometheus</h3><p>学习Prometheus的第一步就是先部署一个实验环境，官方提供了多种方式进行Prometheus安装：</p>
<ul>
<li>源码编译安装</li>
<li>下载预编译好的二进制文件</li>
<li>使用Docker部署</li>
<li>使用第三方工具：Ansible、SaltStack、Puppet、Chef。</li>
</ul>
<p>为了方便学习，首先我们使用二进制方式部署，可以在这里<a target="_blank" rel="noopener external nofollow noreferrer" href="https://prometheus.io/download/%E4%B8%8B%E8%BD%BD%E5%AF%B9%E5%BA%94%E7%9A%84%E9%A2%84%E7%BC%96%E8%AF%91%E7%9A%84%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E3%80%82">https://prometheus.io/download/下载对应的预编译的二进制文件。</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd &#x2F;usr&#x2F;local&#x2F;src</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# wget</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;prometheus&#x2F;releases&#x2F;download&#x2F;v2.7.1&#x2F;prometheus-2.7.1.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# tar zxf prometheus-2.7.1.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# mv prometheus-2.7.1.linux-amd64 &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# ln -s &#x2F;usr&#x2F;local&#x2F;prometheus-2.7.1.linux-amd64&#x2F;</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;prometheus</span><br></pre></td></tr></table></figure>

<p><strong>Prometheus配置</strong></p>
<p>Prometheus的配置文件在prometheus.yml中，直接启动也会到命令的当前目录下寻找该文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd &#x2F;usr&#x2F;local&#x2F;prometheus</span><br><span class="line"></span><br><span class="line">[root@linux-node1 prometheus]# vim prometheus.yml</span><br><span class="line"></span><br><span class="line"># my global config</span><br><span class="line"></span><br><span class="line">global:</span><br><span class="line">scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"># scrape_timeout is set to the global default (10s).</span><br><span class="line"># Alertmanager configuration</span><br><span class="line">alerting:</span><br><span class="line">alertmanagers:</span><br><span class="line">- static_configs:</span><br><span class="line">- targets:</span><br><span class="line"># - alertmanager:9093</span><br><span class="line"># Load rules once and periodically evaluate them according to the global</span><br><span class="line">&#39;evaluation_interval&#39;.</span><br><span class="line">rule_files:</span><br><span class="line"># - &quot;first_rules.yml&quot;</span><br><span class="line"># - &quot;second_rules.yml&quot;</span><br><span class="line"># A scrape configuration containing exactly one endpoint to scrape:</span><br><span class="line"># Here it&#39;s Prometheus itself.</span><br><span class="line">scrape_configs:</span><br><span class="line"># The job name is added as a label &#96;job&#x3D;&lt;job_name&gt;&#96; to any timeseries</span><br><span class="line">scraped from this config.</span><br><span class="line">- job_name: &#39;prometheus&#39;</span><br><span class="line"># metrics_path defaults to &#39;&#x2F;metrics&#39;</span><br><span class="line"># scheme defaults to &#39;http&#39;.</span><br><span class="line">static_configs:</span><br><span class="line">- targets: [&#39;localhost:9090&#39;]</span><br></pre></td></tr></table></figure>

<p><strong>启动Prometheus</strong></p>
<p>默认情况下Prometheus会把数据写在启动目录的./data目录下，可以通过启动参数指定目录：–storage.tsdb.path=”data/“，更多参数可以通过—help查看</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 prometheus]# .&#x2F;prometheus –help</span><br><span class="line"></span><br><span class="line">[root@linux-node1 prometheus]# .&#x2F;prometheus</span><br><span class="line"></span><br><span class="line">…</span><br><span class="line"></span><br><span class="line">level&#x3D;info ts&#x3D;2019-02-12T08:04:03.799169159Z caller&#x3D;main.go:620 msg&#x3D;&quot;Starting</span><br><span class="line">TSDB ...&quot;</span><br><span class="line">level&#x3D;info ts&#x3D;2019-02-12T08:04:03.835497463Z caller&#x3D;main.go:635 msg&#x3D;&quot;TSDB</span><br><span class="line">started&quot;</span><br><span class="line">level&#x3D;info ts&#x3D;2019-02-12T08:04:03.835598421Z caller&#x3D;main.go:695 msg&#x3D;&quot;Loading</span><br><span class="line">configuration file&quot; filename&#x3D;prometheus.yml</span><br><span class="line">level&#x3D;info ts&#x3D;2019-02-12T08:04:03.83756508Z caller&#x3D;main.go:722 msg&#x3D;&quot;Completed</span><br><span class="line">loading of configuration file&quot; filename&#x3D;prometheus.yml</span><br><span class="line">level&#x3D;info ts&#x3D;2019-02-12T08:04:03.83760078Z caller&#x3D;main.go:589 msg&#x3D;&quot;Server is</span><br><span class="line">ready to receive web requests.&quot;</span><br><span class="line">level&#x3D;info ts&#x3D;2019-02-12T08:04:03.837641772Z caller&#x3D;web.go:416 component&#x3D;web</span><br><span class="line">msg&#x3D;&quot;Start listening for connections&quot; address&#x3D;0.0.0.0:9090</span><br></pre></td></tr></table></figure>

<p>默认会在前台启动，并监听9090端口，会自动创建data目录，并存放数据。注意如何服务器时间不正确会有警告提示，请保证服务器时间同步。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/266a101825cbabc2782820895e161f59.png" alt="img"></p>
<p><strong>放置在后台运行</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# groupadd prometheus</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# useradd -g prometheus -d &#x2F;var&#x2F;lib&#x2F;prometheus -s</span><br><span class="line">&#x2F;sbin&#x2F;nologin prometheus</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;prometheus.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line"></span><br><span class="line">Description&#x3D;prometheus</span><br><span class="line">After&#x3D;network.target</span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">User&#x3D;prometheus</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;prometheus</span><br><span class="line">--config.file&#x3D;&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;prometheus.yml</span><br><span class="line">--storage.tsdb.path&#x3D;&#x2F;var&#x2F;lib&#x2F;prometheus</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure>

<p>后台启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# systemctl enable prometheus</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# systemctl start prometheus</span><br></pre></td></tr></table></figure>

<p>查看启动状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# netstat -ntlp | grep 9090</span><br><span class="line"></span><br><span class="line">tcp6 0 0 :::9090 :::* LISTEN 61333&#x2F;prometheus</span><br></pre></td></tr></table></figure>

<h3 id="使用Node-Exporter采集主机数据"><a href="#使用Node-Exporter采集主机数据" class="headerlink" title="使用Node Exporter采集主机数据"></a>使用Node Exporter采集主机数据</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# cd &#x2F;usr&#x2F;local&#x2F;src</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# wget</span><br><span class="line">&lt;https:&#x2F;&#x2F;github.com&#x2F;prometheus&#x2F;node_exporter&#x2F;releases&#x2F;download&#x2F;v0.17.0&#x2F;node_exporter-0.17.0.linux-amd64.tar.gz&gt;</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# tar zxf node_exporter-0.17.0.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# mv node_exporter-0.17.0.linux-amd64 &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line"></span><br><span class="line">[root@linux-node1 src]# ln -s &#x2F;usr&#x2F;local&#x2F;node_exporter-0.17.0.linux-amd64&#x2F;</span><br><span class="line">&#x2F;usr&#x2F;local&#x2F;node_exporter</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;node_exporter.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;node_exporter</span><br><span class="line">After&#x3D;network.target</span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">User&#x3D;prometheus</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;node_exporter&#x2F;node_exporter</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure>

<p>启动Node Exporter</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# systemctl enable node_exporter</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# systemctl start node_exporter</span><br></pre></td></tr></table></figure>

<p>查看状态<br>[root@linux-node1 ~]# netstat -ntlp | grep 9100</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcp6 0 0 :::9100 :::* LISTEN 66239&#x2F;node_exporter</span><br></pre></td></tr></table></figure>

<p>默认情况下Node Exporter监听9100端口，通过/metrics暴露采集到的监控数据，Prometheus默认也从该地址获取数据。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/1004a69a33423c72a2989005be5a790e.png" alt="img"></p>
<p><strong>配置Prometheus读取Node Exporter数据</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim &#x2F;usr&#x2F;local&#x2F;prometheus&#x2F;prometheus.yml</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line"># The job name is added as a label &#96;job&#x3D;&lt;job_name&gt;&#96; to any timeseries</span><br><span class="line">scraped from this config.</span><br><span class="line">- job_name: &#39;prometheus&#39;</span><br><span class="line"># metrics_path defaults to &#39;&#x2F;metrics&#39;</span><br><span class="line"># scheme defaults to &#39;http&#39;.</span><br><span class="line">static_configs:</span><br><span class="line">- targets: [&#39;localhost:9090&#39;]</span><br><span class="line">- job_name: &#39;linux-node1&#39;</span><br><span class="line">static_configs:</span><br><span class="line">- targets: [&#39;192.168.56.11:9100&#39;]</span><br><span class="line">labels:</span><br><span class="line">instance: linux-node1</span><br></pre></td></tr></table></figure>

<p><strong>重启prometheus</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# systemctl restart prometheus</span><br></pre></td></tr></table></figure>

<p><strong>查看监控状态</strong></p>
<p>登录Prometheus的Web控制台，StatusTargets如果可以linux-node1并且状态是UP的状态即为配置成功。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/8c3da60dbf5558dc649de1fd6ce43bf0.png" alt="img"></p>
<h3 id="使用Prometheus-UI查看数据"><a href="#使用Prometheus-UI查看数据" class="headerlink" title="使用Prometheus UI查看数据"></a>使用Prometheus UI查看数据</h3><p>现在Prometheus会定期的从<a target="_blank" rel="noopener external nofollow noreferrer" href="http://192.168.56.11:9100/metrics%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%B9%B6%E5%AD%98%E5%82%A8%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8Prometheus">http://192.168.56.11:9100/metrics获取数据，并存储，我们可以使用Prometheus</a> UI来查看监控数据。</p>
<h3 id="使用Grafana进行数据可视化"><a href="#使用Grafana进行数据可视化" class="headerlink" title="使用Grafana进行数据可视化"></a>使用Grafana进行数据可视化</h3><p><strong>1.安装Grafana</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim &#x2F;etc&#x2F;yum.repos.d&#x2F;grafana.repo</span><br><span class="line"></span><br><span class="line">[grafana]</span><br><span class="line">name&#x3D;grafana</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;packages.grafana.com&#x2F;oss&#x2F;rpm</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;packages.grafana.com&#x2F;gpg.key</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# yum install -y grafana</span><br></pre></td></tr></table></figure>

<p><strong>2.配置Grafana</strong></p>
<p>Grafana的配置文件在/etc/grafana/grafana.ini，默认情况下Grafana监听3000端口</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim &#x2F;etc&#x2F;grafana&#x2F;grafana.ini</span><br></pre></td></tr></table></figure>

<p><strong>3.启动Grafana</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# systemctl enable grafana-server</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# systemctl start grafana-server</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# netstat -ntlp | grep 3000</span><br><span class="line"></span><br><span class="line">tcp6 0 0 :::3000 :::* LISTEN 81427&#x2F;grafana-serve</span><br></pre></td></tr></table></figure>

<p><strong>4.访问Grafana</strong></p>
<p>访问<a target="_blank" rel="noopener external nofollow noreferrer" href="http://192.168.56.11:3000/">http://192.168.56.11:3000</a>，用户名和密码默认为admin/admin，第一次登陆会要求修改密码，请使用安全密码。</p>
<p><strong>5.增加Prometheus数据源</strong></p>
<p>点击</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/b681a9b528d2ff21ba66666ce2452e51.png" alt="img"></p>
<p>，然后选择</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/d3e83ac4f090a51c5b5e0c341b99dda5.png" alt="img"></p>
<p>。</p>
<p>配置URL为：<a target="_blank" rel="noopener external nofollow noreferrer" href="http://192.168.56.11:9090/">http://192.168.56.11:9090</a>，并点击Save&amp;Test。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/e35d8aaebedd7e168ebd1b29b65b30bb.png" alt="img"></p>
<p><strong>6.设置Dashboard</strong></p>
<p>数据源设置完毕后，就可以设置Dashboard图形展示，可以手动添加，也可以直接下载别人配置好保持的Json文件直接导入即可。</p>
<p>下载地址：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://grafana.com/dashboards/405%EF%BC%8C%E5%9C%A8%E5%8F%B3%E4%BE%A7%E6%9C%89Download">https://grafana.com/dashboards/405，在右侧有Download</a> Json按钮，下载该Json文件。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/d9ab69b29a964a12df52512a7b128b5b.png" alt="img"></p>
<p>点击Home下的Import Dashboard</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/53ffc0e739ca7b9421f9568ae4cbf117.png" alt="img"></p>
<p>然后直接上传刚才下载的JSON文件。</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/500958891a82067b0c987d514239ffb0.png" alt="img"></p>
<p>导入完毕后，就可以在Grafana上查看对应节点的监控数据图表。你可以通过鼠标拖拽进行图表的自定义大小和位置的修改，效果如下：</p>
<p><img src="http://k8s.unixhot.com/kubernetes/media/698a6241faa0adc1af6c09cc369b259b.png" alt="img"></p>
<h1 id="第七部分-Kubernetes高级进阶"><a href="#第七部分-Kubernetes高级进阶" class="headerlink" title="第七部分 Kubernetes高级进阶"></a>第七部分 Kubernetes高级进阶</h1><h1 id="Kubernetes的权限控制RBAC"><a href="#Kubernetes的权限控制RBAC" class="headerlink" title="Kubernetes的权限控制RBAC"></a>Kubernetes的权限控制RBAC</h1><p><strong>角色</strong></p>
<ul>
<li><p>Role: 角色，命名空间范围内的一个权限集合。</p>
</li>
<li><p>ClusterRole：集群角色，集群范围内的一个权限的集合，</p>
<p>Role和ClusterROle在Kubernetes中都被定义为集群内部的 API 资源，和我们前面学习过的 Pod、ConfigMap 这些类似，都是我们集群的资源对象，所以同样的可以使用我们前面的kubectl相关的命令来进行操作 Subject：主题，对应在集群中尝试操作的对象，集群中定义了3种类型的主题资源：</p>
</li>
</ul>
<p>User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用 KeyStone或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理 Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如cluster-admin Service Account：服务帐号，通过Kubernetes API 来管理的一些用户帐号，和 namespace 进行关联的，适用于集群内部运行的应用程序，需要通过 API 来完成权限认证，所以在集群内部进行权限操作，我们都需要使用到 ServiceAccount，这也是我们这节课的重点 RoleBinding 和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的 Subject 和我们的 Role 进行绑定的过程(给某个用户绑定上操作的权限)，二者的区别也是作用范围的区别：RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而 ClusterRoleBinding 会影响到所有的 namespace。</p>
<p><strong>创建用户凭证</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# openssl genrsa -out jenkins.key 2048</span><br><span class="line">[root@linux-node1 ~]# openssl req -new -key jenkins.key -out jenkins.csr -subj &quot;&#x2F;CN&#x3D;jenkins&#x2F;O&#x3D;vmware&quot;</span><br><span class="line">[root@linux-node1 ~]# openssl x509 -req -in jenkins.csr -CA &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt -CAkey &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.key -CAcreateserial -out jenkins.crt -days 365</span><br><span class="line">[root@linux-node1 ~]# kubectl config set-credentials jenkins --client-certificate&#x3D;jenkins.crt  --client-key&#x3D;jenkins.key</span><br><span class="line">[root@linux-node1 ~]# kubectl config set-context jenkins-context --cluster&#x3D;kubernetes --namespace&#x3D;jenkins --user&#x3D;jenkins  </span><br><span class="line">[root@linux-node1 ~]# kubectl get pods --context&#x3D;jenkins-context</span><br></pre></td></tr></table></figure>

<p><strong>创建角色</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim jenkins-role.yml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins-role</span><br><span class="line">  namespace: jenkins</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;, &quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">  resources: [&quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br></pre></td></tr></table></figure>

<p><strong>创建角色绑定</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# vim jenkins-role-binding.yml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins-rolebinding</span><br><span class="line">  namespace: jenkins</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: jenkins</span><br><span class="line">  apiGroup: &quot;&quot;</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: jenkins-role</span><br><span class="line">  apiGroup: &quot;&quot;</span><br></pre></td></tr></table></figure>

<h1 id="22-深入理解Pod调度"><a href="#22-深入理解Pod调度" class="headerlink" title="22 深入理解Pod调度"></a>22 深入理解Pod调度</h1><h1 id="深入理解Pod调度"><a href="#深入理解Pod调度" class="headerlink" title="深入理解Pod调度"></a>深入理解Pod调度</h1><p>在前面的章节我们已经知道在Kubernetes中使用kube-scheduler进行Pod调度，它的目标是将Pod绑定到对应的Node上，经过一系列的条件和算法尽可能的让每个Pod都满意。kube-scheduler是Kubernetes默认的调度器。</p>
<p>kube-scheduler的代码位于<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler">GitHub</a></p>
<p>可以将代码克隆到本地方便查看</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# git clone --depth 1 https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes.git</span><br></pre></td></tr></table></figure>

<p>在algorithm下有调度算法，调度算法分为两个阶段：Predicates和priorities，首先对Node进行过滤看哪些Node符合调度要求，然后在符合调度要求的Node上进行优先级计算，判断调度到哪个Node最合适。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 algorithm]# pwd</span><br><span class="line">&#x2F;root&#x2F;kubernetes&#x2F;pkg&#x2F;scheduler&#x2F;algorithm</span><br><span class="line">[root@linux-node1 algorithm]# ls -l</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r-- 1 root root 1256 Dec 17 22:52 BUILD</span><br><span class="line">-rw-r--r-- 1 root root  735 Dec 17 22:52 doc.go</span><br><span class="line">drwxr-xr-x 2 root root  276 Dec 17 22:52 predicates</span><br><span class="line">drwxr-xr-x 3 root root 4096 Dec 17 22:52 priorities</span><br><span class="line">-rw-r--r-- 1 root root 3278 Dec 17 22:52 scheduler_interface.go</span><br><span class="line">-rw-r--r-- 1 root root 3383 Dec 17 22:52 types.go</span><br></pre></td></tr></table></figure>

<p>官方文档详细的介绍了所有的步骤：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/">https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/</a></p>
<p><strong>设置调度器</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get pod kube-proxy-5wbtf -n kube-system -o yaml | grep schedulerName</span><br><span class="line">  schedulerName: default-scheduler</span><br></pre></td></tr></table></figure>

<h2 id="Taints（污点）"><a href="#Taints（污点）" class="headerlink" title="Taints（污点）"></a>Taints（污点）</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl describe node linux-node1.unixhot.com | grep Taints</span><br><span class="line">Taints:             node-role.kubernetes.io&#x2F;master:NoSchedule</span><br></pre></td></tr></table></figure>

<p>Taints的表现形式为</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;key&gt;&#x3D;&lt;value&gt;:&lt;effect&gt;</span><br></pre></td></tr></table></figure>

<p>effect的三种类型：</p>
<ul>
<li>NoSchedule: 如果Pod没有容忍该污点，不调度到该节点上。</li>
<li>PreferNoSchedule：尽量阻止Pod被调度到这个节点上，但是如果没有其它节点能够调度，可以调度到该节点。</li>
<li>NoExecute： NoScheduler和PreferNoSchedule只是在调度阶段起作用，但是NoExecute会影响正常运行的Pod，如果一个节点被打了NoExecute的污点，而运行在该节点的Pod没有容忍会直接被这个节点移除。</li>
</ul>
<p>查看Flannel为何能调度到Master节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl get po -n kube-system | grep flannel</span><br><span class="line">kube-flannel-ds-amd64-f2jrk                       1&#x2F;1     Running   2          22h</span><br><span class="line">kube-flannel-ds-amd64-mh75v                       1&#x2F;1     Running   2          22h</span><br><span class="line">kube-flannel-ds-amd64-n52zm                       1&#x2F;1     Running   4          22h</span><br><span class="line"></span><br><span class="line">[root@linux-node1 ~]# kubectl describe pod kube-flannel-ds-amd64-f2jrk -n kube-system</span><br><span class="line">...</span><br><span class="line">Tolerations:     :NoSchedule</span><br><span class="line">                 node.kubernetes.io&#x2F;disk-pressure:NoSchedule</span><br><span class="line">                 node.kubernetes.io&#x2F;memory-pressure:NoSchedule</span><br><span class="line">                 node.kubernetes.io&#x2F;network-unavailable:NoSchedule</span><br><span class="line">                 node.kubernetes.io&#x2F;not-ready:NoExecute</span><br><span class="line">                 node.kubernetes.io&#x2F;pid-pressure:NoSchedule</span><br><span class="line">                 node.kubernetes.io&#x2F;unreachable:NoExecute</span><br><span class="line">                 node.kubernetes.io&#x2F;unschedulable:NoSchedule</span><br></pre></td></tr></table></figure>

<h3 id="自定义污点"><a href="#自定义污点" class="headerlink" title="自定义污点"></a>自定义污点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[root@linux-node1 ~]# kubectl taint node linux-node2.example.com node-ytpe&#x3D;gpu:NoSchedule       </span><br><span class="line">node&#x2F;linux-node2.example.com tainted</span><br><span class="line">[root@linux-node1 example]# cat nginx-deployment-taint.yaml    </span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.13.12</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-type</span><br><span class="line">        operator: Equal</span><br><span class="line">        value: gpu</span><br><span class="line">        effect: Noschedule</span><br><span class="line">[root@linux-node1 example]# kubectl get po -o wide</span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE     IP          NODE                      NOMINATED NODE   READINESS GATES</span><br><span class="line">dns-test                            1&#x2F;1     Running   1          6h15m   10.2.2.23   linux-node3.example.com   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-deployment-77564d4546-2jkw9   1&#x2F;1     Running   0          13s     10.2.2.26   linux-node3.example.com   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-deployment-77564d4546-4hrbf   1&#x2F;1     Running   0          13s     10.2.2.24   linux-node3.example.com   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-deployment-77564d4546-s2r4h   1&#x2F;1     Running   0          13s     10.2.2.25   linux-node3.example</span><br></pre></td></tr></table></figure>

<h2 id="亲缘性调度"><a href="#亲缘性调度" class="headerlink" title="亲缘性调度"></a>亲缘性调度</h2><h1 id="23-Kubernetes-API介绍"><a href="#23-Kubernetes-API介绍" class="headerlink" title="23 Kubernetes API介绍"></a>23 Kubernetes API介绍</h1><p>查看集群状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https:&#x2F;&#x2F;192.168.56.11:6443</span><br><span class="line">KubeDNS is running at https:&#x2F;&#x2F;192.168.56.11:6443&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br></pre></td></tr></table></figure>

<p>直接访问Kubernetes API需要验证，无法直接访问。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# curl -k https:&#x2F;&#x2F;192.168.56.11:6443</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;forbidden: User \&quot;system:anonymous\&quot; cannot get path \&quot;&#x2F;\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>通过Proxy访问Kubernetes API</strong></p>
<p>使用kubectl proxy可以在Master本地启动一个代理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl proxy</span><br><span class="line">Starting to serve on 127.0.0.1:8001</span><br></pre></td></tr></table></figure>

<p>可以通过127.0.0.1:8001与API Server进行交互</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# curl http:&#x2F;&#x2F;127.0.0.1:8001</span><br><span class="line">&#123;</span><br><span class="line">  &quot;paths&quot;: [</span><br><span class="line">    &quot;&#x2F;api&quot;,</span><br><span class="line">    &quot;&#x2F;api&#x2F;v1&quot;,</span><br><span class="line">    &quot;&#x2F;apis&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;admissionregistration.k8s.io&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;admissionregistration.k8s.io&#x2F;v1beta1&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;apiextensions.k8s.io&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;apiextensions.k8s.io&#x2F;v1beta1&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;apiregistration.k8s.io&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;apiregistration.k8s.io&#x2F;v1&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;apiregistration.k8s.io&#x2F;v1beta1&quot;,</span><br><span class="line">    &quot;&#x2F;apis&#x2F;apps&quot;,</span><br><span class="line">...（省略其它输出）</span><br></pre></td></tr></table></figure>

<p>可以通过修改监听地址，并关闭过滤，实现在其它地方登录和查看，这样就可以在本地浏览器访问API。切记不要再生产环境将代理地址暴露在外网。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl proxy --address&#x3D;0.0.0.0 --disable-filter&#x3D;true</span><br><span class="line">W1105 16:18:45.669591   16730 proxy.go:142] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious</span><br><span class="line">Starting to serve on [::]:8001</span><br></pre></td></tr></table></figure>

<h3 id="使用Swagger-UI进行API交互"><a href="#使用Swagger-UI进行API交互" class="headerlink" title="使用Swagger UI进行API交互"></a>使用Swagger UI进行API交互</h3><p>Kubernetes支持Swagger UI访问API，需要在API Server开启，如果已经根据本书使用kubeadm部署的集群，可以通过修改Pod的YAML文件，重建Pod来开启</p>
<p><strong>修改API Server的Pod定义文件</strong></p>
<p>在- kube-apiserver下面一行增加–enable-swagger-ui=true</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# vim &#x2F;etc&#x2F;kubernetes&#x2F;manifests&#x2F;kube-apiserver.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    component: kube-apiserver</span><br><span class="line">    tier: control-plane</span><br><span class="line">  name: kube-apiserver</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - kube-apiserver</span><br><span class="line">    - --enable-swagger-ui&#x3D;true</span><br><span class="line">    - --advertise-address&#x3D;192.168.56.11</span><br><span class="line">    - --allow-privileged&#x3D;true</span><br></pre></td></tr></table></figure>

<p>删除Pod，kubelet会通过该YAML重建Pod</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl get pod -n kube-system | grep api</span><br><span class="line">kube-apiserver-linux-node1.unixhot.com            1&#x2F;1     Running   0          55m</span><br><span class="line">[root@k8s-master1 ~]# kubectl delete pod kube-apiserver-linux-node1.unixhot.com -n kube-system</span><br></pre></td></tr></table></figure>

<p>可以看到配置已经生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl describe pod kube-apiserver-linux-node1.unixhot.com -n kube-system </span><br><span class="line">...</span><br><span class="line">    Command:</span><br><span class="line">      kube-apiserver</span><br><span class="line">      --enable-swagger-ui&#x3D;true</span><br><span class="line">      --advertise-address&#x3D;192.168.99.27</span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<p>部署一个Swagger UI服务查看API</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 ~]# kubectl run swagger-ui --image&#x3D;swaggerapi&#x2F;swagger-ui:latest</span><br><span class="line">[root@k8s-master1 ~]# kubectl expose deployment swagger-ui --port&#x3D;8080 --type&#x3D;NodePort</span><br><span class="line">[root@k8s-master1 ~]# kubectl get service</span><br><span class="line">NAME         TYPE        CLUSTER-IP    EXTERNAL-IP     PORT(S)          AGE</span><br><span class="line">kubernetes   ClusterIP   10.1.0.1      &lt;none&gt;          443&#x2F;TCP          43d</span><br><span class="line">swagger-ui   NodePort    10.1.205.94   &lt;none&gt;   8080:30410&#x2F;TCP   34s</span><br></pre></td></tr></table></figure>

<p>因为我们部署的Swagger UI和API Server不在一个域名下，所以会有跨域的问题，Chrome浏览器需要提前安装Allow CROS插件解决</p>
<hr>
<hr>
<p>摘录自赵班长——–</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" rel="external nofollow noreferrer">Thaons</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kococ.cn/posts/47211/">https://kococ.cn/posts/47211/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kococ.cn" target="_blank">北青永恒</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/K8s/">K8s</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/xoxoyun/img/raw/master/image/0122.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/5563/"><img class="prev-cover" src="https://gitee.com/xoxoyun/img/raw/master/image/0042.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Git命令集合~</div></div></a></div><div class="next-post pull-right"><a href="/posts/15099/"><img class="next-cover" src="https://gitee.com/xoxoyun/img/raw/master/image/0035.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Docker+K8s实践指南</div></div></a></div></nav></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2020 By Thaons</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">既来之 则安之</div><div class="icp"><a target="_blank" rel="noopener external nofollow noreferrer" href="http://beian.miit.gov.cn/"><img class="icp-icon" src="https://gitee.com/xoxoyun/img/raw/master/image/icp.png"/><span>豫ICP备20001100号</span></a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="aplayer no-destroy" data-id="964294791" data-server="netease" data-type="artist" data-fixed="true" data-mini="true" data-listFolded="true" data-order="random" data-preload="none" data-autoplay="true" muted></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config_change',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  $('script[data-pjax]').each(function () {
    $(this).parent().append($(this).remove())
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  if (typeof gtag === 'function') {
    gtag('config', '', {'page_path': window.location.pathname});
  }

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})


document.addEventListener('pjax:send', function () {
  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  $(window).off('scroll')

  //reset readmode
  $('body').hasClass('read-mode') && $('body').removeClass('read-mode')

})</script><script>(function(){
  const bp = document.createElement('script');
  const curProtocol = window.location.protocol.split(':')[0];
  if (curProtocol === 'https'){
  bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
  }
  else{
  bp.src = 'http://push.zhanzhang.baidu.com/push.js';
  }
  bp.dataset.pjax = ''
  const s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(bp, s);
})()</script></div></body></html>